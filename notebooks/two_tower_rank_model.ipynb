{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wogsim/two_tower_recmodel/blob/main/notebooks/two_tower_rank_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb271239",
      "metadata": {
        "id": "eb271239"
      },
      "source": [
        "\n",
        "\n",
        "# Рекомендации на основе Трансформера\n",
        "\n",
        "Этот проект — это система рекомендаций, использующая архитектуру **Трансформер** для анализа истории позитивных действий пользователя.\n",
        "\n",
        "**Цель:** Научить модель эффективно ранжировать товары, которые пользователь, скорее всего, добавит в корзину.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Входные данные (История взаимодействий)\n",
        "\n",
        "Мы будем использовать историю **положительных взаимодействий** пользователя (сессии):\n",
        "* **Клики**\n",
        "* **Добавления в корзину**\n",
        "* **Покупки**\n",
        "\n",
        "(Примечание: Просмотры временно **игнорируются** для упрощения, но могут быть добавлены позже.)\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Двухэтапное Обучение\n",
        "\n",
        "Обучение модели будет состоять из двух фаз: **Pretrain** (Предварительное обучение) и **Finetune** (Тонкая настройка).\n",
        "\n",
        "### А. Pretrain (Предварительное обучение)\n",
        "\n",
        "* **Задача:** Обучить модель решать **общую задачу**, максимально утилизируя **все имеющиеся данные** (клики, корзины, покупки).\n",
        "* **Результат:** Получение сильных общих представлений (эмбеддингов) для пользователей и товаров.\n",
        "* **Свойство:** Хорошо масштабируется — добавление данных повышает качество.\n",
        "\n",
        "### Б. Finetune (Тонкая настройка)\n",
        "\n",
        "* **Задача:** **Адаптировать** модель под **конкретную целевую задачу ранжирования** на тесте.\n",
        "* **Целевая Метрика:** **$ndcg@10$** (Normalized Discounted Cumulative Gain на 10) по группам (`request_id`).\n",
        "* **Метод:** Обучение на **попарную функцию потерь** (Pairwise Loss) — будем использовать **Calibrated Pairwise Logistic** (подробности ниже).\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Целевая Задача Finetune (Ранжирование)\n",
        "\n",
        "На этапе Finetune мы фокусируемся на ранжировании внутри групп (`request_id`).\n",
        "\n",
        "* **Метки:**\n",
        "    * **1 (Позитивный):** Товар был **добавлен в корзину**.\n",
        "    * **0 (Негативный):** Товар был **просмотрен** (но не добавлен в корзину).\n",
        "* **Исключения:** Клики и покупки **не** учитываются, так как их нет в итоговом тестовом наборе.\n",
        "* **Цель:** **Отранжировать** единички (корзины) как можно **выше** ноликов (просмотров)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8affb83",
      "metadata": {
        "id": "e8affb83"
      },
      "source": [
        "### Pretrain:\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://i.ibb.co/8GksnWD/Screenshot-2025-05-04-at-10-15-36.png\" width=\"500\" alt=\"pretrain\">\n",
        "</div>\n",
        "\n",
        "Предварительное обучение состоит из двух взаимодополняющих задач: **Next-Positive Prediction (NPP)** и **Feedback Prediction (FP)**.\n",
        "\n",
        "### Общая структура токена\n",
        "\n",
        "Каждое положительное взаимодействие ($t$) кодируется как **токен**, который является суммой векторов трёх характеристик:\n",
        "$$\\text{Токен}_t = \\mathbf{c}_t + \\mathbf{i}_t + \\mathbf{f}_t$$\n",
        "* $\\mathbf{i}_t$ — **Товар** (Item) взаимодействия.\n",
        "* $\\mathbf{c}_t$ — **Контекст** (Context) взаимодействия.\n",
        "* $\\mathbf{f}_t$ — **Фидбек** (Feedback): клик, корзина или покупка.\n",
        "\n",
        "### Next-Positive Prediction ($\\mathcal{L}_{\\mathrm{NPP}}$)\n",
        "* **Задача:** Предсказать вероятность следующего товара $i_t$, используя Softmax и косинусное сходство:\n",
        "$$\n",
        "P(\\text{item}=i_t\\mid \\text{history}=S_{t-1},\\;\\text{context}=c_t)\\,.\n",
        "$$\n",
        "* **Позднее связывание (Next-Positive):** Трансформер выдает $h_{t-1}$. Для предсказания используем:\n",
        "$$\n",
        "\\hat h^c_t = \\mathrm{MLP}\\bigl(\\mathrm{Concat}(h_{t-1},\\mathbf{c}_t)\\bigr).\n",
        "$$\n",
        "\n",
        "### Feedback Prediction ($\\mathcal{L}_{\\mathrm{FP}}$)\n",
        "* **Задача:** Предсказать вероятность типа фидбека $f_t$ (классификация на три класса: клик, корзина, покупка):\n",
        "$$\n",
        "P(\\text{feedback}=f_t\\mid \\text{history}=S_{t-1},\\;\\text{context}=c_t,\\;\\text{item}=i_t).\n",
        "$$\n",
        "* **Позднее связывание (Feedback):** Используем $h_{t-1}$, $\\mathbf{c}_t$ и $\\mathbf{i}_t$:\n",
        "$$\n",
        "\\hat h^i_t = \\mathrm{MLP}\\bigl(\\mathrm{Concat}(h_{t-1},\\mathbf{c}_t,\\mathbf{i}_t)\\bigr).\n",
        "$$\n",
        "\n",
        "### Итоговый Pretrain Loss\n",
        "\n",
        "$$\\mathcal{L}_{\\rm pre\\text{-}train} = \\mathcal{L}_{\\mathrm{NPP}} + \\mathcal{L}_{\\mathrm{FP}}.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67330ec3",
      "metadata": {
        "id": "67330ec3"
      },
      "source": [
        "### Finetune\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.ibb.co/kgwt7pRb/Screenshot-2025-05-04-at-10-15-48.png\" width=\"500\" alt=\"finetune\">\n",
        "</p>\n",
        "\n",
        "\n",
        "#### Постановка задачи\n",
        "\n",
        "Пусть для пользователя с идентификатором `user_id` есть сформированная история взаимодействий и набор групп товаров, каждая из которых имеет свой `request_id`.\n",
        "\n",
        "* **Скрытые состояния:**\n",
        "    $$h_0, h_1, \\dots, h_t$$\n",
        "* **Группа (`request_id`):** Множество товаров, каждый из которых имеет бинарную метку:\n",
        "    * **1:** Товар был добавлен в корзину (позитив).\n",
        "    * **0:** Товар был просмотрен (негатив).\n",
        "\n",
        "**Цель:** Для каждого товара внутри группы оценить его **релевантность** пользователю, используя актуальное скрытое состояние пользователя.\n",
        "\n",
        "---\n",
        "\n",
        "#### Учет временной задержки (Consistency with Validation)\n",
        "\n",
        "На валидации и тесте между последним известным состоянием истории и моментом показа новой группы может быть значительный разрыв (от 2 дней до 1 месяца). Чтобы имитировать это и обеспечить консистентность, мы добавляем задержку в процесс обучения:\n",
        "\n",
        "1.  **Выбор задержки:** Для каждой группы случайно выбирается задержка $\\Delta$:\n",
        "$$\\Delta \\sim \\mathrm{Uniform}(2\\ \\text{дн.},\\ 32 \\text{дн.}).$$\n",
        "2.  **Выбор состояния:** Находим самое *позднее* скрытое состояние $h_k$, которое предшествует текущей группе *не менее* чем на $\\Delta$.\n",
        "3.  **Вектор пользователя:** Используем это состояние **$h_k$** как итоговый вектор пользователя для расчета потерь в данной группе.\n",
        "\n",
        "---\n",
        "\n",
        "#### Попарная ранжирующая функция потерь\n",
        "\n",
        "Мы обучаем модель ранжировать позитивные товары выше негативных в рамках одной группы.\n",
        "\n",
        "1.  **Формирование пар:** Внутри каждой группы (`request_id`) генерируем все возможные пары товаров $(i, j)$, где:\n",
        "    * Товар $i$ имеет метку **1** (корзина, позитив).\n",
        "    * Товар $j$ имеет метку **0** (просмотр, негатив).\n",
        "2.  **Расчет потерь:** Для каждой такой пары рассчитываем ранжирующую функцию потерь (например, **Calibrated Pairwise Logistic**), используя предсказанные релевантности для товаров $i$ и $j$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690d22b4",
      "metadata": {
        "id": "690d22b4"
      },
      "source": [
        "## 2. Предобработка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "IxE0DgCs0CoG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxE0DgCs0CoG",
        "outputId": "af6a40ec-c7b0-4020-c5ea-e3a0f78508a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'two_tower_recmodel' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/wogsim/two_tower_recmodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "tPiUHHE50Kem",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tPiUHHE50Kem",
        "outputId": "d5e81dd7-c1eb-4f2e-e4e3-c1aa5fd6cc81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from -r /content/two_tower_recmodel/notebooks/requirements.txt (line 1)) (1.25.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r /content/two_tower_recmodel/notebooks/requirements.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r /content/two_tower_recmodel/notebooks/requirements.txt (line 4)) (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 4)) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 4)) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 4)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->-r /content/two_tower_recmodel/notebooks/requirements.txt (line 2)) (3.0.2)\n",
            "Obtaining file:///content/two_tower_recmodel/grocery\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: catboost>=1.2.5 in /usr/local/lib/python3.12/dist-packages (from grocery==0.1.0) (1.2.8)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from grocery==0.1.0) (1.5.2)\n",
            "Requirement already satisfied: matplotlib>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from grocery==0.1.0) (3.10.6)\n",
            "Requirement already satisfied: numpy~=1.26.4 in /usr/local/lib/python3.12/dist-packages (from grocery==0.1.0) (1.26.4)\n",
            "Requirement already satisfied: polars>=1.22.0 in /usr/local/lib/python3.12/dist-packages (from grocery==0.1.0) (1.25.2)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from grocery==0.1.0) (2.32.4)\n",
            "Requirement already satisfied: scikit-learn>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from grocery==0.1.0) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.15.2 in /usr/local/lib/python3.12/dist-packages (from grocery==0.1.0) (1.16.2)\n",
            "Requirement already satisfied: tqdm>=4.67.1 in /usr/local/lib/python3.12/dist-packages (from grocery==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: voyager>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from grocery==0.1.0) (2.1.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost>=1.2.5->grocery==0.1.0) (0.21)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost>=1.2.5->grocery==0.1.0) (2.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost>=1.2.5->grocery==0.1.0) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost>=1.2.5->grocery==0.1.0) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.1->grocery==0.1.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.1->grocery==0.1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.1->grocery==0.1.0) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.1->grocery==0.1.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.1->grocery==0.1.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.1->grocery==0.1.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.1->grocery==0.1.0) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.1->grocery==0.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->grocery==0.1.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->grocery==0.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->grocery==0.1.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->grocery==0.1.0) (2025.8.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.1->grocery==0.1.0) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost>=1.2.5->grocery==0.1.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost>=1.2.5->grocery==0.1.0) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost>=1.2.5->grocery==0.1.0) (8.5.0)\n",
            "Building wheels for collected packages: grocery\n",
            "  Building editable for grocery (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grocery: filename=grocery-0.1.0-py3-none-any.whl size=1515 sha256=bfd4ef5aaa1d56a73fce1201fff8e4074d57d2fc74b1828b92a6b4c946e736f7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-no_yu498/wheels/1e/69/79/9e835b8b9571913ea43b36767a1872057c821869f21e022ee7\n",
            "Successfully built grocery\n",
            "Installing collected packages: grocery\n",
            "  Attempting uninstall: grocery\n",
            "    Found existing installation: grocery 0.1.0\n",
            "    Uninstalling grocery-0.1.0:\n",
            "      Successfully uninstalled grocery-0.1.0\n",
            "Successfully installed grocery-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r /content/two_tower_recmodel/notebooks/requirements.txt\n",
        "!pip install -e two_tower_recmodel/grocery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7lUF4pHB0Jxq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lUF4pHB0Jxq",
        "outputId": "1f2c54dd-a4bf-42a6-91bf-62f214c121f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "lavka.zip: 100%|██████████████████████████████████████████████████| 447M/447M [00:05<00:00, 75.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unpacking lavka.zip...\n",
            "Files from lavka.zip successfully unpacked\n",
            "\n"
          ]
        }
      ],
      "source": [
        "DATA_DIR = \"/content/recsys_course/data/lavka\"\n",
        "\n",
        "from grocery.utils.dataset import download_and_extract\n",
        "download_and_extract(\n",
        "     url=\"https://www.kaggle.com/api/v1/datasets/download/thekabeton/ysda-recsys-2025-lavka-dataset\",\n",
        "     filename=\"lavka.zip\",\n",
        "    dest_dir=DATA_DIR\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RYxiGoUZ2PT6",
      "metadata": {
        "id": "RYxiGoUZ2PT6"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"/content/recsys_course/data/lavka\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a60653f7",
      "metadata": {
        "id": "a60653f7"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "from collections import deque\n",
        "from typing import Dict, Any, Generator, Iterable, Optional\n",
        "from abc import ABC, abstractmethod\n",
        "from itertools import chain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "614d9df1",
      "metadata": {
        "id": "614d9df1"
      },
      "source": [
        " Оставляем только небольшой поднабор всех признаков. Остальные признаки буду учтены в будущем."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f704462",
      "metadata": {
        "id": "7f704462"
      },
      "outputs": [],
      "source": [
        "train_df = pl.read_parquet(DATA_DIR + '/train.parquet').select(['action_type', 'product_id', 'source_type', 'timestamp', 'user_id', 'request_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8987a472",
      "metadata": {
        "id": "8987a472"
      },
      "source": [
        "### Разделение на трейн и валидация:\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://i.ibb.co/yBPn87t7/IMG000-19.jpg\" width=\"500\" alt=\"split\">\n",
        "</div>\n",
        "\n",
        "Для оценки модели будем использовать train данные. Из них сформируем валидационную и обучающую части. Для того, чтобы получить корректные метрики на валидации, важно повторить все особенности тестовых данных:\n",
        "- 2 дня разница между train и valid\n",
        "- 1 месяц на valid\n",
        "- оставляем только просмотр и корзину\n",
        "- группы с >= 10 товарами\n",
        "- группы с хотя бы одной корзиной и хотя бы одним просмотром"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cde01cf5",
      "metadata": {
        "id": "cde01cf5"
      },
      "outputs": [],
      "source": [
        "class ActionType:\n",
        "    VIEW = 'AT_View'\n",
        "    CLICK = 'AT_Click'\n",
        "    CART_UPDATE = 'AT_CartUpdate'\n",
        "    PURCHASE = 'AT_Purchase'\n",
        "\n",
        "\n",
        "class Preprocessor:\n",
        "    mapping_action_types = {\n",
        "        ActionType.VIEW: 0,\n",
        "        ActionType.CART_UPDATE: 1,\n",
        "        ActionType.CLICK: 2,\n",
        "        ActionType.PURCHASE: 3\n",
        "    }\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_df: pl.DataFrame,\n",
        "    ):\n",
        "        self.train_df = train_df\n",
        "\n",
        "    def _map_col(self, column: str, cast: pl.DataType = None) -> dict:\n",
        "        uniques = sorted(self.train_df.select(pl.col(column)).unique().to_series().to_list())\n",
        "        mapping = {val: idx for idx, val in enumerate(uniques)}\n",
        "\n",
        "        for attr in (\"train_df\",):\n",
        "            df = getattr(self, attr)\n",
        "            df = df.with_columns(\n",
        "                pl.col(column)\n",
        "                .replace(mapping)\n",
        "                .alias(column)\n",
        "            )\n",
        "            if cast is not None:\n",
        "                df = df.with_columns(pl.col(column).cast(cast))\n",
        "            setattr(self, attr, df)\n",
        "\n",
        "        return mapping\n",
        "\n",
        "    def run(self):\n",
        "        self.train_df = self.train_df.with_columns(\n",
        "            pl.col(\"source_type\").fill_null(\"\").alias(\"source_type\")\n",
        "        )\n",
        "\n",
        "        self.mapping_product_ids = self._map_col(\"product_id\")\n",
        "        self.mapping_user_ids = self._map_col(\"user_id\")\n",
        "        self.mapping_source_types = self._map_col(\"source_type\", cast=pl.Int8)\n",
        "\n",
        "        self.train_df = self.train_df.with_columns(\n",
        "            pl.col(\"action_type\")\n",
        "            .replace(self.mapping_action_types)\n",
        "            .cast(pl.Int8)\n",
        "            .alias(\"action_type\")\n",
        "        )\n",
        "\n",
        "        self.targets = (\n",
        "            self.train_df\n",
        "            .filter(\n",
        "                pl.col(\"request_id\").is_not_null() &\n",
        "                pl.col(\"action_type\").is_in([0, 1]) &\n",
        "                (pl.col(\"source_type\") != self.mapping_source_types[\"ST_Catalog\"])\n",
        "            )\n",
        "            .group_by([\n",
        "                \"user_id\",\n",
        "                \"request_id\",\n",
        "                \"product_id\",\n",
        "            ])\n",
        "            .agg([\n",
        "                pl.col(\"action_type\").max(),\n",
        "                pl.col(\"timestamp\").min(),\n",
        "                pl.col(\"source_type\").mode().first()\n",
        "            ])\n",
        "        )\n",
        "\n",
        "        requests_with_cartupdate_and_view = (\n",
        "            self.targets\n",
        "            .select([\"request_id\", \"action_type\", \"timestamp\"])\n",
        "            .group_by(\"request_id\")\n",
        "            .agg([\n",
        "                pl.col(\"action_type\").max().alias(\"max_t\"),\n",
        "                pl.col(\"action_type\").min().alias(\"min_t\"),\n",
        "                pl.len(),\n",
        "                pl.col(\"timestamp\").min().alias(\"req_ts\")\n",
        "            ])\n",
        "            .with_columns(sum_targets=pl.col('max_t').add(pl.col('min_t')))\n",
        "            .filter(pl.col('sum_targets') == 1)\n",
        "            .filter(pl.col('len') >= 10)\n",
        "            .select([\"request_id\", \"req_ts\"])\n",
        "        )\n",
        "        self.targets = (\n",
        "            self.targets\n",
        "            .drop(\"timestamp\")\n",
        "            .join(requests_with_cartupdate_and_view, on=\"request_id\", how=\"inner\")\n",
        "            .with_columns(pl.col(\"req_ts\").alias(\"timestamp\"))\n",
        "            .drop(\"req_ts\")\n",
        "        )\n",
        "        self.targets = (\n",
        "            self.targets\n",
        "            .group_by(['user_id', 'request_id', 'timestamp', 'source_type'])\n",
        "            .agg([\n",
        "                pl.col('product_id'),\n",
        "                pl.col('action_type'),\n",
        "            ])\n",
        "        )\n",
        "\n",
        "        self.timesplit_valid_end = self.train_df[\"timestamp\"].max()\n",
        "        self.timesplit_valid_start = self.timesplit_valid_end - 30 * 24 * 60 * 60\n",
        "        self.timesplit_train_end = self.timesplit_valid_start - 2 * 24 * 60 * 60\n",
        "        self.timesplit_train_start = self.train_df[\"timestamp\"].min()\n",
        "\n",
        "        self.train_df = (\n",
        "            self.train_df\n",
        "            .filter(pl.col(\"action_type\") != 0)\n",
        "            .drop(\"request_id\")\n",
        "        )\n",
        "\n",
        "        self.train_targets = self.targets.filter(\n",
        "            pl.col(\"timestamp\") <= self.timesplit_train_end\n",
        "        )\n",
        "        self.valid_targets = self.targets.filter(\n",
        "            (pl.col(\"timestamp\") > self.timesplit_valid_start) &\n",
        "            (pl.col(\"timestamp\") <= self.timesplit_valid_end)\n",
        "        )\n",
        "        self.train_history = self.train_df.filter(pl.col('timestamp') <= self.timesplit_train_end)\n",
        "        self.valid_history = self.train_df.filter(pl.col('timestamp') > self.timesplit_train_end)\n",
        "\n",
        "        return (\n",
        "            self.train_history,\n",
        "            self.valid_history,\n",
        "            self.train_targets,\n",
        "            self.valid_targets\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3347dba2",
      "metadata": {
        "id": "3347dba2"
      },
      "outputs": [],
      "source": [
        "preprocessor = Preprocessor(train_df)\n",
        "train_history, valid_history, train_targets, valid_targets = preprocessor.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd7ada58",
      "metadata": {
        "id": "bd7ada58"
      },
      "source": [
        "- train_history/valid_history - позитивные взаимодействия пользователей\n",
        "- train_targets/valid_targets - группы для finetune"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5773f69f",
      "metadata": {
        "id": "5773f69f"
      },
      "source": [
        "## 3. Подготовка данных для pretrain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2c44cca",
      "metadata": {
        "id": "b2c44cca"
      },
      "source": [
        "Для pretrain и finetune работа происходит с двумя последовательностями: последовательностью позитивных взаимодействий и последовательностью request-ов пользователя. Далее будем называть их history и candidates. На pretrain нам будет нужна только history.\n",
        "\n",
        "#### Обрезание историй\n",
        "\n",
        "У пользователей может быть разное количество позитивных событий в истории. Для простоты будет рассматривать последние 512 событий. Если вдруг их будет больше, то будем обрезать.\n",
        "\n",
        "#### Схемы таблиц и пример\n",
        "\n",
        "Схема для history имеет вид:\n",
        "```python\n",
        "HISTORY_SCHEMA = pl.Struct([{\n",
        "    'source_type': pl.List(pl.Int64),\n",
        "    'action_type': pl.List(pl.Int64),\n",
        "    'product_id': pl.List(pl.Int64),\n",
        "    'position': pl.List(pl.Int64),\n",
        "    'targets_inds': pl.List(pl.Int64),\n",
        "    'targets_lengths': pl.List(pl.Int64), # количество таргет событий в истории\n",
        "    'lengths': pl.List(pl.Int64), # длина всей истории\n",
        "}]).\n",
        "```\n",
        "`position` это индексы событий в истории (нужно будут далее для позиционных эмбеддингов), `targets_inds` - индексы тех позиций, которые будут участвовать в подсчете функции потерь. Они нужны, чтобы разделять потерю по событиям из обучающий и валидационной частей. `targets_lengths` - количество таких событий в истории.\n",
        "Пример: Пусть есть некоторый пользователь с историей из позитивных взаимодействий длины 5. Пусть первые 3 события попадают в обучение, а последние 2 в валидацию. Тогда:\n",
        "```python\n",
        "history_train_sample = pl.DataFrame([{\n",
        "    'source_type': [1, 1, 2, 3, 4],\n",
        "    'action_type': [1, 0, 1, 0, 1],\n",
        "    'product_id': [1, 2, 3, 4, 5],\n",
        "    'position': [0, 1, 2, 3, 4],\n",
        "    'targets_inds': [0, 1, 2],\n",
        "    'targets_lengths': [3],\n",
        "    'lengths': [5]\n",
        "}])\n",
        "history_valid_sample = pl.DataFrame([{\n",
        "    'source_type': [1, 1, 2, 3, 4],\n",
        "    'action_type': [1, 0, 1, 0, 1],\n",
        "    'product_id': [1, 2, 3, 4, 5],\n",
        "    'position': [0, 1, 2, 3, 4],\n",
        "    'targets_inds': [3, 4],\n",
        "    'targets_lengths': [2],\n",
        "    'lengths': [5]\n",
        "}])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d6b6709",
      "metadata": {
        "id": "0d6b6709"
      },
      "outputs": [],
      "source": [
        "def ensure_sorted_by_timestamp(group: Iterable[Dict[str, Any]]) -> Generator[Dict[str, Any], None, None]:\n",
        "    \"\"\"\n",
        "    Ensures that the given iterable of events is sorted by the 'timestamp' field.\n",
        "\n",
        "    This function iterates over each event in the provided iterable and checks if the\n",
        "    'timestamp' of the current event is greater than or equal to the 'timestamp' of the\n",
        "    previous event. If any event has a 'timestamp' that is less than the previous event's\n",
        "    'timestamp', an AssertionError is raised.\n",
        "\n",
        "    @param group: An iterable of dictionaries, where each dictionary represents an event with at least a 'timestamp' key.\n",
        "    @return: A generator yielding each event from the input iterable in order, ensuring they are sorted by 'timestamp'.\n",
        "    @raises AssertionError: If the events are not sorted by 'timestamp'.\n",
        "    \"\"\"\n",
        "\n",
        "    events = chain(group)\n",
        "\n",
        "    prev_timestamp = 0\n",
        "    for event in events:\n",
        "        if event[\"timestamp\"] >= prev_timestamp:\n",
        "            prev_timestamp = event[\"timestamp\"]\n",
        "            yield event\n",
        "        else:\n",
        "            raise AssertionError(\"Events are not sorted by timestamp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6949bd9a",
      "metadata": {
        "id": "6949bd9a"
      },
      "outputs": [],
      "source": [
        "class Mapper(ABC):\n",
        "    HISTORY_SCHEMA = pl.Struct({\n",
        "        'source_type': pl.List(pl.Int64),\n",
        "        'action_type': pl.List(pl.Int64),\n",
        "        'product_id': pl.List(pl.Int64),\n",
        "        'position': pl.List(pl.Int64),\n",
        "        'targets_inds': pl.List(pl.Int64),\n",
        "        'targets_lengths': pl.List(pl.Int64), # количество таргет событий в истории\n",
        "        'lengths': pl.List(pl.Int64), # длина всей истории\n",
        "    })\n",
        "    CANDIDATES_SCHEMA = pl.Struct({\n",
        "        'source_type': pl.List(pl.Int64),\n",
        "        'action_type': pl.List(pl.Int64),\n",
        "        'product_id': pl.List(pl.Int64),\n",
        "        'lengths': pl.List(pl.Int64), # длина каждого реквеста\n",
        "        'num_requests': pl.List(pl.Int64) # общее количество реквестов у этого пользователя\n",
        "    })\n",
        "\n",
        "    def __init__(self, min_length: int, max_length: int):\n",
        "        self._min_length: int = min_length\n",
        "        self._max_length: int = max_length\n",
        "\n",
        "    @abstractmethod\n",
        "    def __call__(self, group: pl.DataFrame) -> pl.DataFrame:\n",
        "        pass\n",
        "\n",
        "    def get_empty_frame(self, candidates=False):\n",
        "        return pl.DataFrame(schema=pl.Schema({\n",
        "            'history': self.HISTORY_SCHEMA,\n",
        "            **({'candidates': self.CANDIDATES_SCHEMA} if candidates else {})\n",
        "        }))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34414554",
      "metadata": {
        "id": "34414554"
      },
      "source": [
        "Структура, которая:\n",
        "- накапливает history для пользователя и оставлять последние `max_length`\n",
        "- умеет обращаться по индексу к событию истории\n",
        "- имеет метод `get(self, targets_ids)`, который превращает `self._data` в dict в соотвествии со схемой `HISTORY_SCHEMA`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1c9597d",
      "metadata": {
        "id": "e1c9597d"
      },
      "outputs": [],
      "source": [
        "class HistoryDeque:\n",
        "    def __init__(self, max_length=512):\n",
        "        self._data = deque([], maxlen=max_length)\n",
        "\n",
        "    def append(self, x):\n",
        "        self._data.append(x)\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self._data))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self._data[idx]\n",
        "\n",
        "    def get(self, targets_inds=None):\n",
        "        \"\"\"\n",
        "        Retrieves a dictionary containing various attributes of the dataset samples.\n",
        "\n",
        "        If `targets_inds` is not provided, it automatically identifies indices of samples where the `target` is 1.\n",
        "\n",
        "        @param targets_inds: List of indices of target samples. If None, it will be determined based on samples with target value 1.\n",
        "        @return: Dictionary with keys ['source_type', 'action_type', 'product_id', 'position', 'targets_inds', 'targets_lengths', 'lengths']\n",
        "                Each key maps to a list or value representing the respective attribute of the dataset samples.\n",
        "        \"\"\"\n",
        "        if targets_inds is None:\n",
        "            targets_inds = [i for i, value in enumerate(self._data)\n",
        "                                        if value['target'] == 1]\n",
        "\n",
        "        history = {'source_type': [stori['source_type'] for stori in self._data],\n",
        "                    'action_type': [stori['action_type'] for stori in self._data],\n",
        "                    'product_id': [stori['product_id'] for stori in self._data],\n",
        "                    'position': list(range(len(self._data))),\n",
        "                    'targets_inds': targets_inds,\n",
        "                    'targets_lengths': [len(targets_inds)],\n",
        "                    'lengths': [len(self._data)]}\n",
        "\n",
        "\n",
        "        return history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49cfc0dc",
      "metadata": {
        "id": "49cfc0dc"
      },
      "source": [
        "На основе функции `get_pretrain_data` реализуем `PretrainMapper`, который будет по данном пользователю выдавать обучающий пример в нужном формате."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dd0a3f4",
      "metadata": {
        "id": "7dd0a3f4"
      },
      "outputs": [],
      "source": [
        "class PretrainMapper(Mapper):\n",
        "    def __call__(self, group: pl.DataFrame) -> pl.DataFrame:\n",
        "        \"\"\"\n",
        "        Processes a group of data by maintaining a history of rows up to a specified maximum length.\n",
        "        If the history meets the minimum length requirement and contains at least one target, it returns\n",
        "        a DataFrame with the history. Otherwise, it returns an empty DataFrame.\n",
        "\n",
        "        @param group: A Polars DataFrame containing the group of data to process.\n",
        "        @return: A Polars DataFrame containing the history if conditions are met; otherwise, an empty DataFrame.\n",
        "        \"\"\"\n",
        "        deque = HistoryDeque(self._max_length)\n",
        "        events_generator = ensure_sorted_by_timestamp(group.to_struct())\n",
        "\n",
        "        for event in events_generator:\n",
        "            deque.append(event)\n",
        "\n",
        "        if len(deque) > self._min_length:\n",
        "            return pl.DataFrame([{'history': deque.get()}], schema=pl.Schema({'history': Mapper.HISTORY_SCHEMA}))\n",
        "\n",
        "        else:\n",
        "            return self.get_empty_frame()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d41d08d",
      "metadata": {
        "id": "5d41d08d"
      },
      "outputs": [],
      "source": [
        "def get_pretrain_data(train_history: pl.DataFrame,\n",
        "                      valid_history: pl.DataFrame,\n",
        "                      min_length: int = 5,\n",
        "                      max_length: int = 4096) -> pl.DataFrame:\n",
        "    mapper = PretrainMapper(\n",
        "        min_length=min_length,\n",
        "        max_length=max_length,\n",
        "    )\n",
        "\n",
        "    train_data = (\n",
        "        train_history.with_columns(target=pl.lit(1))\n",
        "        .sort(['user_id', 'timestamp'])\n",
        "        .group_by('user_id')\n",
        "        .map_groups(mapper)\n",
        "    )\n",
        "\n",
        "    valid_data = (\n",
        "        pl.concat([\n",
        "            train_history.with_columns(target=pl.lit(0)),\n",
        "            valid_history.with_columns(target=pl.lit(1))\n",
        "        ], how='diagonal')\n",
        "        .sort(['user_id', 'timestamp'])\n",
        "        .group_by('user_id')\n",
        "        .map_groups(mapper)\n",
        "    )\n",
        "\n",
        "    return train_data, valid_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d7d7425",
      "metadata": {
        "id": "9d7d7425"
      },
      "outputs": [],
      "source": [
        "pretrain_train_data, pretrain_valid_data = get_pretrain_data(train_history, valid_history, min_length=5, max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8hWVOKSwq77J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "collapsed": true,
        "id": "8hWVOKSwq77J",
        "outputId": "503cf797-1cdc-4f45-fa53-6b892f413a98"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (2_099, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>history</th></tr><tr><td>struct[7]</td></tr></thead><tbody><tr><td>{[1, 4, … 0],[1, 2, … 3],[2594, 14908, … 3651],[0, 1, … 182],[180, 181, 182],[3],[183]}</td></tr><tr><td>{[1, 1, … 1],[2, 2, … 1],[19210, 8368, … 10986],[0, 1, … 6],[5, 6],[2],[7]}</td></tr><tr><td>{[1, 1, … 1],[1, 1, … 2],[248, 24235, … 4297],[0, 1, … 39],[],[0],[40]}</td></tr><tr><td>{[3, 3, … 12],[1, 1, … 1],[11151, 6673, … 15581],[0, 1, … 39],[],[0],[40]}</td></tr><tr><td>{[1, 1, … 0],[1, 1, … 3],[11905, 8728, … 5651],[0, 1, … 102],[68, 69, … 102],[35],[103]}</td></tr><tr><td>&hellip;</td></tr><tr><td>{[8, 8, … 0],[1, 1, … 3],[11122, 15697, … 22773],[0, 1, … 15],[],[0],[16]}</td></tr><tr><td>{[8, 8, … 8],[2, 2, … 1],[9902, 9902, … 10053],[0, 1, … 78],[78],[1],[79]}</td></tr><tr><td>{[1, 0, … 0],[1, 3, … 3],[10964, 10964, … 24734],[0, 1, … 31],[],[0],[32]}</td></tr><tr><td>{[1, 1, … 0],[1, 1, … 3],[4612, 14635, … 14635],[0, 1, … 5],[],[0],[6]}</td></tr><tr><td>{[1, 1, … 1],[2, 1, … 1],[19384, 19384, … 9968],[0, 1, … 50],[],[0],[51]}</td></tr></tbody></table></div>"
            ],
            "text/plain": [
              "shape: (2_099, 1)\n",
              "┌─────────────────────────────────┐\n",
              "│ history                         │\n",
              "│ ---                             │\n",
              "│ struct[7]                       │\n",
              "╞═════════════════════════════════╡\n",
              "│ {[1, 4, … 0],[1, 2, … 3],[2594… │\n",
              "│ {[1, 1, … 1],[2, 2, … 1],[1921… │\n",
              "│ {[1, 1, … 1],[1, 1, … 2],[248,… │\n",
              "│ {[3, 3, … 12],[1, 1, … 1],[111… │\n",
              "│ {[1, 1, … 0],[1, 1, … 3],[1190… │\n",
              "│ …                               │\n",
              "│ {[8, 8, … 0],[1, 1, … 3],[1112… │\n",
              "│ {[8, 8, … 8],[2, 2, … 1],[9902… │\n",
              "│ {[1, 0, … 0],[1, 3, … 3],[1096… │\n",
              "│ {[1, 1, … 0],[1, 1, … 3],[4612… │\n",
              "│ {[1, 1, … 1],[2, 1, … 1],[1938… │\n",
              "└─────────────────────────────────┘"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pretrain_valid_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d18672be",
      "metadata": {
        "id": "d18672be"
      },
      "source": [
        "## 4. Реализуем свой torch.nn.utils.data.Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da2be818",
      "metadata": {
        "id": "da2be818"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import polars as pl\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5764444",
      "metadata": {
        "id": "d5764444"
      },
      "outputs": [],
      "source": [
        "def convert_dict_to_tensor(data_dict):\n",
        "    \"\"\"\n",
        "    Recursively converts lists within a dictionary to PyTorch tensors with dtype=torch.int64.\n",
        "\n",
        "    @param data_dict: A dictionary potentially containing nested dictionaries and lists.\n",
        "    @return: A new dictionary with the same structure as `data_dict`, but with lists converted to PyTorch tensors.\n",
        "    \"\"\"\n",
        "    if not isinstance(data_dict, dict):\n",
        "        if isinstance(data_dict, str):\n",
        "            return data_dict\n",
        "        return torch.tensor(data_dict, dtype=torch.int64)\n",
        "    else:\n",
        "        new_dict = {}\n",
        "        for key in data_dict:\n",
        "            new_dict[key] = convert_dict_to_tensor(data_dict[key])\n",
        "    return new_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc80b148",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc80b148",
        "outputId": "7067fc43-2fd0-40dc-b2d5-bd76a10c8e44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': tensor([1, 2, 3]), 'b': {'c': tensor([4, 5]), 'd': tensor(6)}, 'e': 'text'}\n"
          ]
        }
      ],
      "source": [
        "def test_convert_dict_to_tensor_basic():\n",
        "    input_data = {\n",
        "        'a': [1, 2, 3],\n",
        "        'b': {\n",
        "            'c': [4, 5],\n",
        "            'd': 6\n",
        "        },\n",
        "        'e': 'text'\n",
        "    }\n",
        "\n",
        "    result = convert_dict_to_tensor(input_data)\n",
        "    print(result)\n",
        "\n",
        "    assert isinstance(result['a'], torch.Tensor)\n",
        "    assert result['a'].dtype == torch.int64\n",
        "    assert result['a'].tolist() == [1, 2, 3]\n",
        "\n",
        "    assert isinstance(result['b'], dict)\n",
        "    assert isinstance(result['b']['c'], torch.Tensor)\n",
        "    assert result['b']['c'].dtype == torch.int64\n",
        "    assert result['b']['c'].tolist() == [4, 5]\n",
        "\n",
        "    assert result['b']['d'] == 6\n",
        "    assert result['e'] == 'text'\n",
        "\n",
        "test_convert_dict_to_tensor_basic()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f256020",
      "metadata": {
        "id": "4f256020"
      },
      "outputs": [],
      "source": [
        "class LavkaDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataframe(cls, df: pl.DataFrame) -> 'LavkaDataset':\n",
        "        converted_data = [convert_dict_to_tensor(group) for group in df.to_struct()]\n",
        "\n",
        "        return cls(converted_data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96b9e3b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96b9e3b2",
        "outputId": "1ab425a1-c49a-4f42-f482-b3c81e647de4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating train dataset ...\n",
            "Creating valid dataset ...\n"
          ]
        }
      ],
      "source": [
        "print(\"Creating train dataset ...\")\n",
        "train_ds = LavkaDataset.from_dataframe(pretrain_train_data)\n",
        "print(\"Creating valid dataset ...\")\n",
        "valid_ds = LavkaDataset.from_dataframe(pretrain_valid_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96837a5f",
      "metadata": {
        "id": "96837a5f"
      },
      "source": [
        "## 5. Реализуем основной backbone модели"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11db0387",
      "metadata": {
        "id": "11db0387"
      },
      "source": [
        "Реализуем класс ResNet-блока согласно следующим формулам:\n",
        "\n",
        "Для входа $x\\in\\mathbb{R}^{\\text{batch}\\times d}$ вычислить:  \n",
        "$$\n",
        "    \\begin{aligned}\n",
        "    z &= xW + b,\\\\\n",
        "    a &= \\mathrm{ReLU}(z),\\\\\n",
        "    d'&= \\mathrm{Dropout}(a),\\\\\n",
        "    y &= \\mathrm{LayerNorm}\\bigl(x + d'\\bigr).\n",
        "    \\end{aligned}\n",
        "$$  \n",
        "В компактном виде:  \n",
        "$$\n",
        "    y = \\mathrm{LayerNorm}\\Bigl(x + \\mathrm{Dropout}\\bigl(\\mathrm{ReLU}(xW + b)\\bigr)\\Bigr)\\,.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "756010dd",
      "metadata": {
        "id": "756010dd"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, embedding_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        identity = x\n",
        "        out = self.linear(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.layer_norm(identity + out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb46dcf3",
      "metadata": {
        "id": "cb46dcf3"
      },
      "outputs": [],
      "source": [
        "def test_resnet_output_shape():\n",
        "    embedding_dim = 32\n",
        "    batch_size = 8\n",
        "    model = ResNet(embedding_dim)\n",
        "    x = torch.randn(batch_size, embedding_dim)\n",
        "    out = model(x)\n",
        "    assert out.shape == (batch_size, embedding_dim)\n",
        "\n",
        "test_resnet_output_shape()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6016c09a",
      "metadata": {
        "id": "6016c09a"
      },
      "source": [
        "Реализуем `ContextEncoder`, `ItemEncoder` и `ActionEncoder`. На вход они принимают torch.tensor с индексами размера (seq_len,), а на выходе получают torch.tensor с соотвествующими векторами размера (seq_len, embedding_dim)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MkvpCmih840Q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkvpCmih840Q",
        "outputId": "754bf473-c84b-4f3b-8e09-ea96fdc24fe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "длинна action 4\n",
            "длинна item 26522\n",
            "длинна source 13\n"
          ]
        }
      ],
      "source": [
        "print(f'длинна action {len(preprocessor.mapping_action_types)}')\n",
        "print(f'длинна item {len(preprocessor.mapping_product_ids)}')\n",
        "print(f'длинна source {len(preprocessor.mapping_source_types)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf716125",
      "metadata": {
        "id": "cf716125"
      },
      "outputs": [],
      "source": [
        "class ContextEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim=64):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(13, embedding_dim)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.embeddings(inputs)\n",
        "\n",
        "\n",
        "class ItemEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim=64):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(26522, embedding_dim)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.embeddings(inputs)\n",
        "\n",
        "\n",
        "class ActionEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim=64):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(4, embedding_dim)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.embeddings(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_seq_len: int = 512, embedding_dim: int= 64):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_len, embedding_dim)\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim)\n",
        "        )\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, pos: torch.Tensor) -> torch.Tensor:\n",
        "        return self.pe[0].index_select(0, pos.flatten())"
      ],
      "metadata": {
        "id": "-bk60mvsGe4V"
      },
      "id": "-bk60mvsGe4V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "111d85db",
      "metadata": {
        "id": "111d85db"
      },
      "outputs": [],
      "source": [
        "def test_context_encoder_shape():\n",
        "    embedding_dim = 16\n",
        "    batch_size = 5\n",
        "    seq_len = 7\n",
        "    model = ContextEncoder(embedding_dim=embedding_dim)\n",
        "    x = torch.randint(0, 4, (batch_size, seq_len))\n",
        "    out = model(x)\n",
        "    assert out.shape == (batch_size, seq_len, embedding_dim)\n",
        "test_context_encoder_shape()\n",
        "\n",
        "def test_item_encoder_shape():\n",
        "    embedding_dim = 20\n",
        "    batch_size = 6\n",
        "    seq_len = 10\n",
        "    model = ItemEncoder(embedding_dim=embedding_dim)\n",
        "    x = torch.randint(0, 20000, (batch_size, seq_len))\n",
        "    out = model(x)\n",
        "    assert out.shape == (batch_size, seq_len, embedding_dim)\n",
        "test_item_encoder_shape()\n",
        "\n",
        "def test_action_encoder_shape():\n",
        "    embedding_dim = 12\n",
        "    batch_size = 4\n",
        "    seq_len = 3\n",
        "    model = ActionEncoder(embedding_dim=embedding_dim)\n",
        "    x = torch.randint(0, 4, (batch_size, seq_len))\n",
        "    out = model(x)\n",
        "    assert out.shape == (batch_size, seq_len, embedding_dim)\n",
        "test_action_encoder_shape()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24183263",
      "metadata": {
        "id": "24183263"
      },
      "outputs": [],
      "source": [
        "def get_mask(lengths):\n",
        "    \"\"\"\n",
        "    Generates a mask tensor based on the given sequence lengths.\n",
        "\n",
        "    The mask is a boolean tensor where each row corresponds to a sequence and contains\n",
        "    True values up to the length of the sequence and False values thereafter.\n",
        "\n",
        "    @param lengths: A 1D tensor containing the lengths of sequences.\n",
        "    @return: A 2D boolean tensor where each row has True up to the corresponding sequence length.\n",
        "    \"\"\"\n",
        "    max_length = max(lengths)\n",
        "    arange_tensor = torch.arange(max_length, device=lengths.device)\n",
        "\n",
        "    return (arange_tensor < lengths.unsqueeze(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa156c03",
      "metadata": {
        "id": "fa156c03"
      },
      "outputs": [],
      "source": [
        "def test_get_mask():\n",
        "    lengths = torch.tensor([2, 3, 1])\n",
        "    expected_mask = torch.tensor([\n",
        "        [True, True, False],\n",
        "        [True, True, True],\n",
        "        [True, False, False]\n",
        "    ])\n",
        "    assert torch.equal(get_mask(lengths), expected_mask)\n",
        "\n",
        "test_get_mask()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f63da84",
      "metadata": {
        "id": "9f63da84"
      },
      "source": [
        "Реализуем `ModelBackbone`. Эта часть модели является общей для pretrain и finetune. Она кодируется входные события, преобразует их в нужный для трансформера формат `(batch_size, seq_len, embedding_dim)`, прогоняет через них трансформер и возвращает три поля: выходы трансформера, вектора товаров и вектора feedback-ов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21f5edfc",
      "metadata": {
        "id": "21f5edfc"
      },
      "outputs": [],
      "source": [
        "class ModelBackbone(nn.Module):\n",
        "    def __init__(self,\n",
        "                 embedding_dim=64,\n",
        "                 num_heads=2,\n",
        "                 max_seq_len=512,\n",
        "                 dropout_rate=0.2,\n",
        "                 num_transformer_layers=2):\n",
        "        super().__init__()\n",
        "        self.context_encoder = ContextEncoder(embedding_dim)\n",
        "        self.item_encoder = ItemEncoder(embedding_dim)\n",
        "        self.action_encoder = ActionEncoder(embedding_dim)\n",
        "        self.position_embeddings = PositionalEncoding(max_seq_len, embedding_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embedding_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=embedding_dim * 4,\n",
        "            dropout=dropout_rate,\n",
        "            activation='gelu',\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_transformer_layers)\n",
        "        self._embedding_dim = embedding_dim\n",
        "\n",
        "    @property\n",
        "    def embedding_dim(self):\n",
        "        return self._embedding_dim\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        context_embeddings = self.context_encoder(inputs['history']['source_type'])\n",
        "        item_embeddings = self.item_encoder(inputs['history']['product_id'])\n",
        "        action_embeddings = self.action_encoder(inputs['history']['action_type'])\n",
        "        position_embedding = self.position_embeddings(inputs['history']['position'])\n",
        "\n",
        "        padding_mask = get_mask(inputs['history']['lengths'])\n",
        "        batch_size, seq_len = padding_mask.shape\n",
        "\n",
        "        token_embeddings = item_embeddings.new_zeros(\n",
        "            batch_size, seq_len, self.embedding_dim, device=context_embeddings.device)\n",
        "\n",
        "        summed_embs = (context_embeddings + item_embeddings\n",
        "                       + action_embeddings + position_embedding)\n",
        "\n",
        "        token_embeddings[padding_mask] = summed_embs\n",
        "\n",
        "        source_embeddings = self.transformer_encoder(\n",
        "            token_embeddings,\n",
        "            mask=torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(device=context_embeddings.device),\n",
        "            src_key_padding_mask=~padding_mask)\n",
        "\n",
        "        return {\n",
        "            'source_embeddings': source_embeddings[padding_mask],\n",
        "            'item_embeddings': item_embeddings.squeeze(),\n",
        "            'context_embeddings': context_embeddings.squeeze()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49b07d58",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49b07d58",
        "outputId": "5beeafd1-7844-4680-906f-d3ab4fd4ca35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 64])\n",
            "torch.Size([5, 64])\n",
            "torch.Size([5, 64])\n"
          ]
        }
      ],
      "source": [
        "def test_model_backbone():\n",
        "    sample = {\n",
        "        'history': {\n",
        "            'source_type': torch.tensor([[1, 1, 7, 1, 1,]]),\n",
        "            'action_type': torch.tensor([[2, 2, 2, 1, 2]]),\n",
        "            'product_id': torch.tensor([[19210,  8368,  5165,  5326, 12476]]),\n",
        "            'position': torch.tensor([[0, 1, 2, 3, 4]]),\n",
        "            'targets_inds': torch.tensor([[0, 1, 2]]),\n",
        "            'targets_lengths': torch.tensor([3]),\n",
        "            'lengths': torch.tensor([5])\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    backbone = ModelBackbone()\n",
        "    output = backbone(sample)\n",
        "    print(output['source_embeddings'].shape)\n",
        "    print(output['item_embeddings'].shape)\n",
        "    print(output['context_embeddings'].shape)\n",
        "    assert output['source_embeddings'].shape == (5, 64)\n",
        "    assert output['item_embeddings'].shape == (5, 64)\n",
        "    assert output['context_embeddings'].shape == (5, 64)\n",
        "\n",
        "test_model_backbone()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30d87649",
      "metadata": {
        "id": "30d87649"
      },
      "source": [
        "## 6. Реализуем pretrain модель"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a95f2cf2",
      "metadata": {
        "id": "a95f2cf2"
      },
      "source": [
        "#### Головы (heads)\n",
        "\n",
        "- **User–context fusion**  \n",
        "      Последовательность ResNet-блоков, свёртка размерности `2D → D`. На входе конкатенация `source_embeddings ∥ context_embeddings`.  \n",
        "- **Candidate projector**  \n",
        "      Три ResNet-блока, преобразующие эмбеддинг товара из `D → D`.  \n",
        "- **Classifier**  \n",
        "      Три ResNet-блока, затем линейный слой `3D → 3`, для предсказания типа следующего действия из трёх возможных (cart, click, purchase).  \n",
        "- **Параметр τ**  \n",
        "      Скаляp-коэффициент `τ = clip(exp(τ_raw), τ_min, τ_max)` для масштабирования скалярных произведений – температура в contrastive-лоссе.\n",
        "\n",
        "\n",
        "#### Формулы лоссов\n",
        "\n",
        "Обозначения:  \n",
        "- $u_i \\in \\mathbb{R}^D$ – нормализованный вектор пользователя для i-го примера.  \n",
        "- $c_i \\in \\mathbb{R}^D$ – нормализованный вектор кандидата (позитивного товара) для i-го примера.  \n",
        "- $\\{n_{ij}\\}_{j=1}^M\\subset\\mathbb{R}^D$ – нормализованные векторы $M$ негативных товаров (весь каталог товаров минус один товар, позитивный).  \n",
        "- $\\text{temp}>0$ – «температура» (скаляр).  \n",
        "- $K= M+1$ – общее число кандидатов (1 позитивный + M негативных). Количество товаров во всем каталоге будет $1 + M$.\n",
        "\n",
        "1. **Retrieval loss** (контрастивный softmax-лосс)  \n",
        "   Для каждого примера $i$ вычисляем скалярные логиты:  \n",
        "   $$\n",
        "     \\ell_i = [\\,\\underbrace{u_i^\\top c_i}_{\\text{позитивный логит}} \\,\\big|\\,\n",
        "               \\underbrace{u_i^\\top n_{i1},\\,u_i^\\top n_{i2},\\,\\dots,\\,u_i^\\top n_{iM}}_{\\text{негативные логиты}}] \\;\\times\\;\\\\text{temp}\n",
        "   $$\n",
        "   Затем лосс  \n",
        "   $$\n",
        "     \\mathcal{L}_{\\mathrm{retr}}\n",
        "     = -\\frac1N \\sum_{i=1}^N \\log\\frac{\\exp\\bigl(u_i^\\top c_i \\,\\text{temp}\\bigr)}\n",
        "                                      {\\exp\\bigl(u_i^\\top c_i \\,\\text{temp}\\bigr)\n",
        "                                     + \\sum_{j=1}^M \\exp\\bigl(u_i^\\top n_{ij}\\,\\text{temp}\\bigr)}.\n",
        "   $$\n",
        "\n",
        "2. **Action loss** (кросс-энтропия)  \n",
        "   Для каждого положительного шага $i$ модель выдаёт логиты $\\mathbf{z}_i \\in \\mathbb{R}^3$ по трём классам действий, а истинная метка $y_i\\in\\{0,1,2\\}$.  \n",
        "   $$\n",
        "     \\mathcal{L}_{\\mathrm{action}}\n",
        "     = -\\frac1N \\sum_{i=1}^N \\sum_{k=0}^2 \\delta_{y_i=k}\\,\\log\\bigl(\\mathrm{softmax}(\\mathbf{z}_i)_k\\bigr),\n",
        "   $$\n",
        "   где $\\mathrm{softmax}(\\mathbf{z})_k = \\frac{e^{z_k}}{\\sum_{m=0}^2 e^{z_m}}$.\n",
        "\n",
        "3. **Итоговый лосс**  \n",
        "   $$\n",
        "     \\mathcal{L}\n",
        "     = \\mathcal{L}_{\\mathrm{retr}}\n",
        "       \\;+\\; 10 \\times \\mathcal{L}_{\\mathrm{action}}.\n",
        "   $$\n",
        "   Перевзвешиваем action часть т.к. у нее сильно меньше масштаб."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "250b7f84",
      "metadata": {
        "id": "250b7f84"
      },
      "outputs": [],
      "source": [
        "class PretrainModel(nn.Module):\n",
        "    MIN_TEMPERATURE = 0.01\n",
        "    MAX_TEMPERATURE = 100\n",
        "\n",
        "    def __init__(self,\n",
        "                 backbone,\n",
        "                 embedding_dim=64):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.user_context_fusion = nn.Sequential(\n",
        "            ResNet(2 * embedding_dim),\n",
        "            ResNet(2 * embedding_dim),\n",
        "            ResNet(2 * embedding_dim),\n",
        "            nn.Linear(2 * embedding_dim, embedding_dim),\n",
        "        )\n",
        "        self.candidate_projector = nn.Sequential(\n",
        "            ResNet(embedding_dim),\n",
        "            ResNet(embedding_dim),\n",
        "            ResNet(embedding_dim),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            ResNet(3 * embedding_dim),\n",
        "            ResNet(3 * embedding_dim),\n",
        "            ResNet(3 * embedding_dim),\n",
        "            nn.Linear(3 * embedding_dim, 3),\n",
        "        )\n",
        "        self._embedding_dim = embedding_dim\n",
        "        self.tau = torch.nn.Parameter(torch.zeros(1, dtype=torch.float32))\n",
        "\n",
        "        self.cross_entr_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    @property\n",
        "    def embedding_dim(self):\n",
        "        return self._embedding_dim\n",
        "\n",
        "    @property\n",
        "    def temperature(self):\n",
        "        return torch.clip(torch.exp(self.tau), min=self.MIN_TEMPERATURE, max=self.MAX_TEMPERATURE)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        backbone_outputs = self.backbone(inputs)\n",
        "        source_embeddings = backbone_outputs['source_embeddings']\n",
        "        item_embeddings = backbone_outputs['item_embeddings']\n",
        "        context_embeddings = backbone_outputs['context_embeddings']\n",
        "\n",
        "        lengths = inputs['history']['lengths']\n",
        "        offsets = torch.cumsum(lengths, dim=0) - lengths\n",
        "        target_mask = torch.full((sum(lengths).item(),), False, dtype=torch.bool, device=lengths.device)\n",
        "        target_inds = torch.repeat_interleave(offsets, inputs['history']['targets_lengths']) + inputs['history']['targets_inds']\n",
        "        target_mask[target_inds] = True\n",
        "        non_first_element = torch.full((sum(lengths).item(),), True, dtype=torch.bool, device=lengths.device)\n",
        "        non_first_element[offsets] = False\n",
        "        source_mask = torch.roll(non_first_element & target_mask, -1)\n",
        "\n",
        "        source_embeddings = source_embeddings[source_mask]\n",
        "        context_embeddings = context_embeddings[non_first_element & target_mask]\n",
        "        item_embeddings = item_embeddings[non_first_element & target_mask]\n",
        "\n",
        "\n",
        "        # calc retrieval loss\n",
        "        user_embeddings = torch.nn.functional.normalize(self.user_context_fusion(torch.cat([source_embeddings, context_embeddings], dim=-1)))\n",
        "        candidate_embeddings = torch.nn.functional.normalize(\n",
        "                                self.candidate_projector(\n",
        "                                self.backbone.item_encoder.embeddings(inputs['history']['product_id'][target_mask & non_first_element])))\n",
        "        negative_embeddings = torch.nn.functional.normalize(self.candidate_projector(self.backbone.item_encoder.embeddings.weight))\n",
        "        pos_logits = torch.sum(user_embeddings * candidate_embeddings, dim=-1) * self.temperature\n",
        "        neg_logits = user_embeddings @ negative_embeddings.T * self.temperature\n",
        "        next_positive_prediction_loss = - torch.mean(\n",
        "                                          (pos_logits) - (torch.logsumexp(neg_logits, dim=-1))\n",
        "                                                  )\n",
        "\n",
        "        # calc action loss\n",
        "        logits = self.classifier(torch.cat([source_embeddings, context_embeddings, item_embeddings], dim=-1))\n",
        "        targets = inputs['history']['action_type'][non_first_element & target_mask] - 1\n",
        "        feedback_prediction_loss = self.cross_entr_loss(logits, targets)\n",
        "\n",
        "        return {\n",
        "            'next_positive_prediction_loss': next_positive_prediction_loss,\n",
        "            'feedback_prediction_loss': feedback_prediction_loss,\n",
        "            'loss': next_positive_prediction_loss + feedback_prediction_loss * 10\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3ff38e6",
      "metadata": {
        "id": "c3ff38e6"
      },
      "source": [
        "## 7. (0.5 балл) Обучим pretrain модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a98d2b65",
      "metadata": {
        "id": "a98d2b65"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Collates a batch of samples from a dataset.\n",
        "\n",
        "    This function is designed to handle batches where each sample is a dictionary.\n",
        "    It recursively collates values associated with the same keys across all samples in the batch.\n",
        "    For tensor values, it concatenates them along the first dimension.\n",
        "    For dictionary values, it applies the same collation logic recursively.\n",
        "    For other types of values, it simply aggregates them into a list.\n",
        "\n",
        "    @param batch: A list of samples, where each sample is a dictionary.\n",
        "    @return: A dictionary with the same keys as the samples, where each value is either\n",
        "             a concatenated tensor, a recursively collated dictionary, or a list of values.\n",
        "    \"\"\"\n",
        "    if isinstance(batch, list) and isinstance(batch[0], dict):\n",
        "        batched_dict = {}\n",
        "        for key in batch[0].keys():\n",
        "            list_of_values = [item[key] for item in batch]\n",
        "            batched_dict[key] = collate_fn(list_of_values)\n",
        "        return batched_dict\n",
        "    elif isinstance(batch, list) and isinstance(batch[0], torch.Tensor):\n",
        "        return torch.cat(batch, dim=0)\n",
        "    elif isinstance(batch, list):\n",
        "        return batch\n",
        "    else:\n",
        "        return batch\n",
        "\n",
        "def move_to_device(batch, device):\n",
        "    \"\"\"\n",
        "    Moves a batch of data to a specified device (e.g., CPU or GPU).\n",
        "\n",
        "    Args:\n",
        "        batch (torch.Tensor or dict): The batch of data to move. Can be a single tensor or a dictionary of tensors.\n",
        "        device (torch.device): The target device to which the batch should be moved.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor or dict: The batch of data moved to the specified device.\n",
        "                             If the input is a dictionary, the returned value will be a dictionary with the same keys\n",
        "                             and values moved to the specified device.\n",
        "    \"\"\"\n",
        "    if isinstance(batch, torch.Tensor):\n",
        "        return batch.to(device)\n",
        "    elif isinstance(batch, dict):\n",
        "        return {key: move_to_device(value, device) for key, value in batch.items()}\n",
        "    elif isinstance(batch, list):\n",
        "        return [move_to_device(item, device) for item in batch]\n",
        "    elif isinstance(batch, tuple):\n",
        "        return tuple(move_to_device(item, device) for item in batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5077c9b9",
      "metadata": {
        "id": "5077c9b9"
      },
      "outputs": [],
      "source": [
        "def test_collate_fn_basic():\n",
        "    batch = [\n",
        "        {\n",
        "            'x': torch.tensor([1, 2]),\n",
        "            'y': {\n",
        "                'z': torch.tensor([[10], [20]]),\n",
        "                'w': 5\n",
        "            },\n",
        "            's': 'foo'\n",
        "        },\n",
        "        {\n",
        "            'x': torch.tensor([3, 4]),\n",
        "            'y': {\n",
        "                'z': torch.tensor([[30], [40]]),\n",
        "                'w': 6\n",
        "            },\n",
        "            's': 'bar'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    result = collate_fn(batch)\n",
        "\n",
        "    assert isinstance(result['x'], torch.Tensor)\n",
        "    assert result['x'].tolist() == [1, 2, 3, 4]\n",
        "\n",
        "    assert isinstance(result['y'], dict)\n",
        "    assert isinstance(result['y']['z'], torch.Tensor)\n",
        "    assert result['y']['z'].tolist() == [[10], [20], [30], [40]]\n",
        "    assert result['y']['w'] == [5, 6]\n",
        "\n",
        "    assert result['s'] == ['foo', 'bar']\n",
        "test_collate_fn_basic()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ef41d30",
      "metadata": {
        "id": "1ef41d30"
      },
      "outputs": [],
      "source": [
        "from statistics import mean\n",
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "\n",
        "def train_pretrain_model(model, train_loader, valid_loader, optimizer, scheduler, num_epochs, device):\n",
        "    global_cnt = 0\n",
        "    prev_valid_loss = None\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        action_losses = []\n",
        "        retrieval_losses = []\n",
        "        for batch in tqdm(train_loader):\n",
        "            batch = move_to_device(batch, device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch)\n",
        "            loss = output['loss']\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "            action_losses.append(output['feedback_prediction_loss'].item())\n",
        "            retrieval_losses.append(output['next_positive_prediction_loss'].item())\n",
        "        scheduler.step()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {mean(train_losses):.6f}, Train Feedback Loss: {mean(action_losses):.6f}, Train NPP Loss: {mean(retrieval_losses):.6f}\")\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        valid_losses = []\n",
        "        action_losses = []\n",
        "        retrieval_losses = []\n",
        "        with torch.inference_mode():\n",
        "            for batch in tqdm(valid_loader):\n",
        "                if len(batch['history']['targets_inds']) == 0:\n",
        "                    continue\n",
        "                batch = move_to_device(batch, device)\n",
        "                output = model(batch)\n",
        "                loss = output['loss']\n",
        "                valid_losses.append(loss.item())\n",
        "                action_losses.append(output['feedback_prediction_loss'].item())\n",
        "                retrieval_losses.append(output['next_positive_prediction_loss'].item())\n",
        "\n",
        "        avg_valid_loss = mean(valid_losses)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Valid Loss: {avg_valid_loss:.6f}, Valid Feedback Loss: {mean(action_losses):.6f}, Valid NPP Loss: {mean(retrieval_losses):.6f}\")\n",
        "\n",
        "        if prev_valid_loss is None or prev_valid_loss > avg_valid_loss:\n",
        "            global_cnt = 0\n",
        "            prev_valid_loss = avg_valid_loss\n",
        "            with torch.no_grad():\n",
        "                torch.save(model, './pretrain.pt')\n",
        "        else:\n",
        "            global_cnt += 1\n",
        "            if global_cnt == 10:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0afde462",
      "metadata": {
        "id": "0afde462"
      },
      "outputs": [],
      "source": [
        "lr = 0.001\n",
        "batch_size = 8\n",
        "warmup_epochs = 4\n",
        "start_factor = 0.1\n",
        "num_epochs = 100\n",
        "\n",
        "embedding_dim = 64\n",
        "num_heads = 2\n",
        "max_seq_len = 512\n",
        "dropout_rate = 0.2\n",
        "num_transformer_layers = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abb8af26",
      "metadata": {
        "id": "abb8af26"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05c8d4a9",
      "metadata": {
        "id": "05c8d4a9"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "backbone = ModelBackbone(embedding_dim=embedding_dim,\n",
        "                        num_heads=num_heads,\n",
        "                        max_seq_len=max_seq_len,\n",
        "                        dropout_rate=dropout_rate,\n",
        "                        num_transformer_layers=num_transformer_layers).to(device)\n",
        "model_pretrain = PretrainModel(backbone=backbone,\n",
        "                             embedding_dim=embedding_dim).to(device)\n",
        "optimizer = optim.AdamW(model_pretrain.parameters(), lr=lr, weight_decay=0.01)\n",
        "scheduler = optim.lr_scheduler.LinearLR(\n",
        "    optimizer,\n",
        "    start_factor=start_factor,\n",
        "    total_iters=warmup_epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8506243",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e8506243",
        "outputId": "2c2c6679-6bb1-4fc0-dbe7-2c00db1c213f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:07<00:00, 31.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 15.076300, Train Feedback Loss: 0.491430, Train NPP Loss: 10.162004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 139.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Valid Loss: 14.385434, Valid Feedback Loss: 0.424745, Valid NPP Loss: 10.137979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:07<00:00, 31.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100, Train Loss: 14.032211, Train Feedback Loss: 0.400547, Train NPP Loss: 10.026740\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 150.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100, Valid Loss: 13.988038, Valid Feedback Loss: 0.407550, Valid NPP Loss: 9.912540\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:07<00:00, 31.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100, Train Loss: 13.579624, Train Feedback Loss: 0.390384, Train NPP Loss: 9.675787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 151.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100, Valid Loss: 13.541099, Valid Feedback Loss: 0.401443, Valid NPP Loss: 9.526667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:07<00:00, 31.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100, Train Loss: 13.109988, Train Feedback Loss: 0.380026, Train NPP Loss: 9.309725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 149.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100, Valid Loss: 13.084324, Valid Feedback Loss: 0.387779, Valid NPP Loss: 9.206531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:07<00:00, 31.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100, Train Loss: 12.581448, Train Feedback Loss: 0.364485, Train NPP Loss: 8.936603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 150.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100, Valid Loss: 12.654987, Valid Feedback Loss: 0.383135, Valid NPP Loss: 8.823638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:07<00:00, 31.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100, Train Loss: 12.023960, Train Feedback Loss: 0.349631, Train NPP Loss: 8.527654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 150.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100, Valid Loss: 12.249575, Valid Feedback Loss: 0.375640, Valid NPP Loss: 8.493171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:07<00:00, 31.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100, Train Loss: 11.568967, Train Feedback Loss: 0.338255, Train NPP Loss: 8.186420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 145.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100, Valid Loss: 11.966042, Valid Feedback Loss: 0.378144, Valid NPP Loss: 8.184600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:07<00:00, 31.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100, Train Loss: 11.096124, Train Feedback Loss: 0.325050, Train NPP Loss: 7.845627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 148.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100, Valid Loss: 11.621952, Valid Feedback Loss: 0.375110, Valid NPP Loss: 7.870847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:07<00:00, 31.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100, Train Loss: 10.697235, Train Feedback Loss: 0.314754, Train NPP Loss: 7.549692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 151.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100, Valid Loss: 11.464827, Valid Feedback Loss: 0.381268, Valid NPP Loss: 7.652151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:07<00:00, 31.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100, Train Loss: 10.411818, Train Feedback Loss: 0.306068, Train NPP Loss: 7.351135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 149.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100, Valid Loss: 11.413508, Valid Feedback Loss: 0.387704, Valid NPP Loss: 7.536468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:07<00:00, 31.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100, Train Loss: 10.231369, Train Feedback Loss: 0.296865, Train NPP Loss: 7.262723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 150.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100, Valid Loss: 11.553783, Valid Feedback Loss: 0.402013, Valid NPP Loss: 7.533655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:07<00:00, 31.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100, Train Loss: 10.075129, Train Feedback Loss: 0.285695, Train NPP Loss: 7.218182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 150.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100, Valid Loss: 11.473783, Valid Feedback Loss: 0.397993, Valid NPP Loss: 7.493850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:07<00:00, 31.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100, Train Loss: 9.935277, Train Feedback Loss: 0.276890, Train NPP Loss: 7.166374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 147.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100, Valid Loss: 11.553828, Valid Feedback Loss: 0.405777, Valid NPP Loss: 7.496060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:07<00:00, 31.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100, Train Loss: 9.794553, Train Feedback Loss: 0.266706, Train NPP Loss: 7.127493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 146.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100, Valid Loss: 11.590894, Valid Feedback Loss: 0.411010, Valid NPP Loss: 7.480794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:07<00:00, 31.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100, Train Loss: 9.652846, Train Feedback Loss: 0.254320, Train NPP Loss: 7.109643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 148.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100, Valid Loss: 11.692090, Valid Feedback Loss: 0.421313, Valid NPP Loss: 7.478960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:07<00:00, 31.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100, Train Loss: 9.539109, Train Feedback Loss: 0.247122, Train NPP Loss: 7.067885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 150.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100, Valid Loss: 11.932073, Valid Feedback Loss: 0.444021, Valid NPP Loss: 7.491863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:07<00:00, 31.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100, Train Loss: 9.405526, Train Feedback Loss: 0.236054, Train NPP Loss: 7.044984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 149.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100, Valid Loss: 12.032263, Valid Feedback Loss: 0.456069, Valid NPP Loss: 7.471575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:07<00:00, 31.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100, Train Loss: 9.294823, Train Feedback Loss: 0.225077, Train NPP Loss: 7.044048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 148.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100, Valid Loss: 12.270269, Valid Feedback Loss: 0.479740, Valid NPP Loss: 7.472866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:07<00:00, 31.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/100, Train Loss: 9.170962, Train Feedback Loss: 0.213693, Train NPP Loss: 7.034028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 146.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/100, Valid Loss: 12.528954, Valid Feedback Loss: 0.504380, Valid NPP Loss: 7.485155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [00:07<00:00, 31.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/100, Train Loss: 9.066911, Train Feedback Loss: 0.203776, Train NPP Loss: 7.029149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 148.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/100, Valid Loss: 12.576748, Valid Feedback Loss: 0.509613, Valid NPP Loss: 7.480619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_pretrain_model(model_pretrain, train_loader, valid_loader, optimizer, scheduler, num_epochs, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2e66695",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2e66695",
        "outputId": "e6db15d1-6660-41b5-a413-8cf53e13ea2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 263/263 [00:01<00:00, 146.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.413507751318125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def best_metric_model(valid_loader):\n",
        "    test_model = torch.load('./pretrain.pt', weights_only=False)\n",
        "    valid_losses = []\n",
        "    with torch.inference_mode():\n",
        "        for batch in tqdm(valid_loader):\n",
        "            if len(batch['history']['targets_inds']) == 0:\n",
        "                    continue\n",
        "            batch = move_to_device(batch, device)\n",
        "            output = test_model(batch)\n",
        "            loss = output['loss']\n",
        "            valid_losses.append(loss.item())\n",
        "    print(mean(valid_losses))\n",
        "\n",
        "best_metric_model(valid_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b660b6c",
      "metadata": {
        "id": "7b660b6c"
      },
      "source": [
        "## 8. Подготовка данных для finetune"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2366cc6",
      "metadata": {
        "id": "d2366cc6"
      },
      "source": [
        "Схемы для candidates:\n",
        "```python\n",
        "CANDIDATES_SCHEMA = pl.Struct({\n",
        "    'source_type': pl.List(pl.Int64),\n",
        "    'action_type': pl.List(pl.Int64),\n",
        "    'product_id': pl.List(pl.Int64),\n",
        "    'lengths': pl.List(pl.Int64), # длина каждого реквеста\n",
        "    'num_requests': pl.List(pl.Int64) # общее количество реквестов у этого пользователя\n",
        "})\n",
        "```\n",
        "\n",
        "Пример семпла:\n",
        "\n",
        "```python\n",
        "finetune_train_sample = {\n",
        "    'history': {...},\n",
        "    'candidates': {\n",
        "        'source_type': [1, 2, 3],\n",
        "        'action_type': [1, 0, 1, 0, 1, 0, 1, 1, 1],\n",
        "        'product_id': [10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
        "        'lengths': [3, 3, 3],\n",
        "        'num_requests': 3\n",
        "    }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10b77da9",
      "metadata": {
        "id": "10b77da9"
      },
      "outputs": [],
      "source": [
        "class Candidates:\n",
        "    def __init__(self, max_requests_size):\n",
        "        self._data = deque([], maxlen=max_requests_size)\n",
        "\n",
        "    def append(self, x):\n",
        "        if x['request_id']:\n",
        "            self._data.append(x)\n",
        "\n",
        "    def popleft(self):\n",
        "        return self._data.popleft()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self._data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._data)\n",
        "\n",
        "    def get(self):\n",
        "        \"\"\"\n",
        "        Aggregates data from the internal _data attribute into a structured dictionary format.\n",
        "\n",
        "        This method constructs a dictionary with keys 'source_type', 'action_type', 'product_id', 'lengths', and 'num_requests'.\n",
        "        - 'source_type' contains the source types from each sample.\n",
        "        - 'action_type' contains all action types from each sample's action_type_list flattened into a single list.\n",
        "        - 'product_id' contains all product IDs from each sample's product_id_list flattened into a single list.\n",
        "        - 'lengths' contains the length of the product_id_list for each sample.\n",
        "        - 'num_requests' contains the total number of samples.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Any]: A dictionary with aggregated data.\n",
        "        \"\"\"\n",
        "\n",
        "        candidate_deque = {'source_type':[],\n",
        "                           'action_type':[],\n",
        "                           'product_id':[],\n",
        "                           'lengths':[],\n",
        "                           'num_requests':[len(self)]}\n",
        "\n",
        "        for x in self:\n",
        "            candidate_deque['source_type'].append(x['source_type'])\n",
        "            candidate_deque['action_type'].extend(x['action_type_list'])\n",
        "            candidate_deque['product_id'].extend(x['product_id_list'])\n",
        "            candidate_deque['lengths'].append(len(x['action_type_list']))\n",
        "\n",
        "        return candidate_deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d13bb58d",
      "metadata": {
        "id": "d13bb58d"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import bisect\n",
        "\n",
        "\n",
        "class FinetuneTrainMapper(Mapper):\n",
        "    def __call__(self, group: pl.DataFrame) -> pl.DataFrame:\n",
        "        \"\"\"\n",
        "        Processes a group of interactions to generate history and candidate sets for recommendation.\n",
        "\n",
        "        This method processes a DataFrame containing interaction data, separating actions into history and candidates based on the presence of 'action_type_list'.\n",
        "        It ensures the data is sorted by timestamp, filters candidates based on time constraints, and selects historical interactions within a specified lag range for each candidate.\n",
        "        If there are no valid candidates or insufficient history, it returns an empty DataFrame.\n",
        "\n",
        "        @param group: A Polars DataFrame containing interaction data with at least 'timestamp' and 'action_type_list' columns.\n",
        "        @return: A Polars DataFrame with 'history' and 'candidates' columns, or an empty DataFrame if no valid candidates are found.\n",
        "        \"\"\"\n",
        "        history_deque = HistoryDeque()\n",
        "        candidate_deque = Candidates(self._max_length)\n",
        "\n",
        "        history_generator = ensure_sorted_by_timestamp(group.to_struct())\n",
        "        for event in history_generator:\n",
        "            if event['action_type_list'] is None:\n",
        "                history_deque.append(event)\n",
        "\n",
        "        candidate_generator = ensure_sorted_by_timestamp(group.to_struct())\n",
        "        for event in candidate_generator:\n",
        "            if event['action_type_list']:\n",
        "                max_time = event['timestamp'] - random.randrange(2, 32) * 86400\n",
        "                target_ind = bisect.bisect_right(history_deque, max_time, key=lambda x: x['timestamp'])\n",
        "                if target_ind == 0:\n",
        "                    continue\n",
        "                event['targets_inds'] = target_ind - 1\n",
        "                candidate_deque.append(event)\n",
        "\n",
        "        targets_inds = [candidate['targets_inds'] for candidate in candidate_deque]\n",
        "\n",
        "\n",
        "        if len(candidate_deque) > self._min_length and len(history_deque) > self._min_length:\n",
        "            return pl.DataFrame([{'history': history_deque.get(targets_inds),\n",
        "                                  'candidates': candidate_deque.get()}],\n",
        "                                 schema=pl.Schema({'history': Mapper.HISTORY_SCHEMA,\n",
        "                                                  'candidates': Mapper.CANDIDATES_SCHEMA}))\n",
        "        else:\n",
        "            return self.get_empty_frame(candidates=True)\n",
        "\n",
        "class FinetuneValidMapper(Mapper):\n",
        "    def __call__(self, group: pl.DataFrame) -> pl.DataFrame:\n",
        "        \"\"\"\n",
        "        Differs only in the formation of target_inds\n",
        "        \"\"\"\n",
        "        history_deque = HistoryDeque()\n",
        "        candidate_deque = Candidates(self._max_length)\n",
        "\n",
        "        history_generator = ensure_sorted_by_timestamp(group.to_struct())\n",
        "        for event in history_generator:\n",
        "            if event['action_type_list'] is None:\n",
        "                history_deque.append(event)\n",
        "\n",
        "        candidate_generator = ensure_sorted_by_timestamp(group.to_struct())\n",
        "        for event in candidate_generator:\n",
        "            if event['action_type_list']:\n",
        "                max_time = event['timestamp']\n",
        "                target_ind = bisect.bisect_right(history_deque, max_time, key=lambda x: x['timestamp'])\n",
        "                if target_ind == 0:\n",
        "                    continue\n",
        "                event['targets_inds'] = target_ind - 1\n",
        "                candidate_deque.append(event)\n",
        "\n",
        "        targets_inds = [candidate['targets_inds'] for candidate in candidate_deque]\n",
        "\n",
        "\n",
        "        if len(candidate_deque) > self._min_length and len(history_deque) > self._min_length:\n",
        "            return pl.DataFrame([{'history': history_deque.get(targets_inds),\n",
        "                                  'candidates': candidate_deque.get()}],\n",
        "                                 schema=pl.Schema({'history': Mapper.HISTORY_SCHEMA,\n",
        "                                                  'candidates': Mapper.CANDIDATES_SCHEMA}))\n",
        "        else:\n",
        "            return self.get_empty_frame(candidates=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "707cfdd7",
      "metadata": {
        "id": "707cfdd7"
      },
      "outputs": [],
      "source": [
        "def get_finetune_data(train_history: pl.DataFrame,\n",
        "                      train_targets: pl.DataFrame,\n",
        "                      valid_targets: pl.DataFrame,\n",
        "                      min_length: int = 5,\n",
        "                      max_length: int = 4096) -> pl.DataFrame:\n",
        "    mapper = FinetuneTrainMapper(\n",
        "        min_length=min_length,\n",
        "        max_length=max_length,\n",
        "    )\n",
        "\n",
        "    train_data = (\n",
        "        pl.concat([\n",
        "            train_history,\n",
        "            train_targets.with_columns([\n",
        "                pl.col('product_id').alias('product_id_list'),\n",
        "                pl.col('action_type').alias('action_type_list')\n",
        "            ]).drop(['product_id', 'action_type'])\n",
        "        ], how='diagonal')\n",
        "        .sort(['user_id', 'timestamp'])\n",
        "        .group_by('user_id')\n",
        "        .map_groups(mapper)\n",
        "    )\n",
        "\n",
        "    mapper = FinetuneValidMapper(\n",
        "        min_length=min_length,\n",
        "        max_length=max_length,\n",
        "    )\n",
        "\n",
        "    valid_data = (\n",
        "        pl.concat([\n",
        "            train_history,\n",
        "            valid_targets.with_columns([\n",
        "                pl.col('product_id').alias('product_id_list'),\n",
        "                pl.col('action_type').alias('action_type_list')\n",
        "            ]).drop(['product_id', 'action_type'])\n",
        "        ], how='diagonal')\n",
        "        .sort(['user_id', 'timestamp'])\n",
        "        .group_by('user_id')\n",
        "        .map_groups(mapper)\n",
        "    )\n",
        "\n",
        "    return train_data, valid_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f663b7c",
      "metadata": {
        "id": "8f663b7c"
      },
      "outputs": [],
      "source": [
        "finetune_train_data, finetune_valid_data = get_finetune_data(train_history, train_targets, valid_targets, min_length=5, max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75788228",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75788228",
        "outputId": "8e0cc96b-dfd6-4454-d6de-cf3a3e4e63f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating train dataset ...\n",
            "Creating valid dataset ...\n"
          ]
        }
      ],
      "source": [
        "print(\"Creating train dataset ...\")\n",
        "train_ds = LavkaDataset.from_dataframe(finetune_train_data)\n",
        "print(\"Creating valid dataset ...\")\n",
        "valid_ds = LavkaDataset.from_dataframe(finetune_valid_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "344bdfff",
      "metadata": {
        "id": "344bdfff"
      },
      "source": [
        "## 9. Реализуем finetune модель"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecbd0611",
      "metadata": {
        "id": "ecbd0611"
      },
      "source": [
        "#### Функция make_groups: разметка элементов по «группам»\n",
        "\n",
        "Дано: вектор длин последовательностей  \n",
        "$$\n",
        "\\mathbf{l} = [\\,l_1, l_2, \\dots, l_B\\,],\\quad l_i\\in\\mathbb{N},\\;\n",
        "B=\\text{batch size}.\n",
        "$$  \n",
        "Нужно получить вектор «номеров групп» длиной  \n",
        "$$\n",
        "N = \\sum_{i=1}^B l_i\n",
        "$$\n",
        "так, чтобы первые $l_1$ элементов имели номер группы 0, следующие $l_2$ - номер 1 и т.д.  \n",
        "Результат:  \n",
        "$$\n",
        "\\mathrm{groups} = [\\,\\underbrace{0,\\dots,0}_{l_1},\\;\n",
        "\\underbrace{1,\\dots,1}_{l_2},\\;\\dots\\;,\\underbrace{B-1,\\dots,B-1}_{l_B}\\,]\\,.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cdaabe5",
      "metadata": {
        "id": "3cdaabe5"
      },
      "outputs": [],
      "source": [
        "def make_groups(lengths: torch.Tensor) -> torch.Tensor:\n",
        "    range_tensor = torch.arange(0, len(lengths), device=lengths.device)\n",
        "    return torch.repeat_interleave(range_tensor, lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ee5be06",
      "metadata": {
        "id": "5ee5be06"
      },
      "outputs": [],
      "source": [
        "def test_make_groups_basic():\n",
        "    lengths = torch.tensor([2, 3, 1])\n",
        "    expected = torch.tensor([0, 0, 1, 1, 1, 2])\n",
        "    result = make_groups(lengths)\n",
        "    assert torch.equal(result, expected)\n",
        "\n",
        "test_make_groups_basic()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "925b500d",
      "metadata": {
        "id": "925b500d"
      },
      "source": [
        "#### Функция make_pairs: построение всех упорядоченных пар внутри групп\n",
        "\n",
        "Цель: для каждого «блока» длины $l_i$ сгенерировать всех $l_i\\times l_i$ упорядоченных пар индексов  \n",
        "$$\n",
        "( p, q ),\\quad p,q\\in\\{0,\\dots,l_i-1\\},\n",
        "$$\n",
        "а затем «развернуть» их по всему батчу. Результат - двумерный тензор shape $(2,\\,\\sum_i l_i^2)$, где\n",
        "\n",
        "- первая строка `pairs` - индексы «первого» элемента пары в пределах своего блока,  \n",
        "- вторая строка `pairs` - индексы «второго».\n",
        "\n",
        "Математически пары нумеруются так:\n",
        "$$\n",
        "\\{\\, (p,q)\\;\\big|\\;p=0..l_i-1,\\;q=0..l_i-1\\;\\}\\quad\\forall i=1..B.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92121880",
      "metadata": {
        "id": "92121880"
      },
      "outputs": [],
      "source": [
        "def make_pairs(lengths: torch.Tensor) -> torch.Tensor:\n",
        "    num_pairs_per_group = lengths**2\n",
        "    total_pairs = torch.sum(num_pairs_per_group)\n",
        "\n",
        "    group_idx = make_groups(num_pairs_per_group)\n",
        "\n",
        "    pair_offsets = torch.cumsum(num_pairs_per_group, dim=0) - num_pairs_per_group\n",
        "    local_pair_idx = torch.arange(total_pairs, device=lengths.device) - pair_offsets.repeat_interleave(num_pairs_per_group)\n",
        "\n",
        "    local_p = local_pair_idx // lengths[group_idx]\n",
        "    local_q = local_pair_idx % lengths[group_idx]\n",
        "\n",
        "    offsets = torch.cumsum(lengths, dim=0) - lengths\n",
        "    global_offsets = offsets[group_idx]\n",
        "\n",
        "    pairs_first = local_p + global_offsets\n",
        "    pairs_second = local_q + global_offsets\n",
        "\n",
        "    return torch.stack([pairs_first, pairs_second], dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cebd057",
      "metadata": {
        "id": "0cebd057"
      },
      "outputs": [],
      "source": [
        "def test_make_pairs_simple():\n",
        "    lengths = torch.tensor([1, 2], dtype=torch.long)\n",
        "    expected = torch.tensor([\n",
        "        [0, 1, 1, 2, 2],\n",
        "        [0, 1, 2, 1, 2]\n",
        "    ], dtype=torch.long)\n",
        "\n",
        "    pairs = make_pairs(lengths)\n",
        "    assert pairs.shape == (2, 5)\n",
        "    assert torch.equal(pairs, expected)\n",
        "\n",
        "test_make_pairs_simple()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d085421",
      "metadata": {
        "id": "3d085421"
      },
      "source": [
        "#### Класс CalibratedPairwiseLogistic: попарная калиброванная логистическая функция потерь\n",
        "\n",
        "Идея была предложена здесь: [Calibrated Pairwise Logistic](https://arxiv.org/pdf/2211.01494). Пусть у нас есть:\n",
        "\n",
        "- логиты всех элементов: $\\mathbf{c} \\in \\mathbb{R}^N$,\n",
        "- таргеты $\\mathbf{t}\\in\\mathbb{R}^N$,\n",
        "\n",
        "Шаги:\n",
        "\n",
        "1. Генерируем все упорядоченные пары индексов внутри групп:  \n",
        "   $$\n",
        "   \\mathrm{pairs} = \\bigl[\\;I_0,\\;I_1\\bigr],\\quad\n",
        "   I_0,I_1\\in\\{0,\\dots,N-1\\}\n",
        "   $$\n",
        "2. Для каждой пары извлекаем  \n",
        "   $$\n",
        "   c_i = c_{I_0},\\quad c_j = c_{I_1},\\quad\n",
        "   t_i = t_{I_0},\\quad t_j = t_{I_1}.\n",
        "   $$\n",
        "3. Отбираем только «положительные» пары, где $t_i > t_j$. Вводим индикатор  \n",
        "   $$\n",
        "   w_{ij} =\n",
        "     \\begin{cases}\n",
        "       1,&t_i > t_j,\\\\\n",
        "       0,&\\text{иначе}.\n",
        "     \\end{cases}\n",
        "   $$\n",
        "   И считаем $W=\\sum w_{ij}$.\n",
        "4. Если $W>0$, вычисляем попарный loss для каждой положительной пары:\n",
        "   \n",
        "   а) сначала вычисляем «калиброванную вероятность» того, что $i$ лучше $j$:\n",
        "   $$\n",
        "     p_{ij}\n",
        "     = \\frac{\\sigma(c_i)}{\\sigma(c_i)+\\sigma(c_j)},\n",
        "     \\quad\n",
        "     \\sigma(x)=\\frac1{1+e^{-x}}.\n",
        "   $$\n",
        "   б) берём отрицательный логарифм правдоподобия:\n",
        "   $$\n",
        "     \\ell_{ij}\n",
        "     = -\\log p_{ij}\n",
        "     = -\\log\\frac{\\sigma(c_i)}{\\sigma(c_i)+\\sigma(c_j)}.\n",
        "   $$\n",
        "   \n",
        "5. Итоговая loss - усреднённая:\n",
        "   $$\n",
        "     \\mathcal{L}\n",
        "     = \\frac{1}{W}\\sum_{i,j} w_{ij}\\;\\ell_{ij}.\n",
        "   $$\n",
        "6. Если $W=0$ (нет ни одной пары с $t_i>t_j$), возвращаем нуль.\n",
        "\n",
        "Таким образом, CalibratedPairwiseLogistic минимизирует  \n",
        "$$\n",
        "-\\frac{1}{W}\\sum_{t_i>t_j}\\log\\frac{\\sigma(c_i)}{\\sigma(c_i)+\\sigma(c_j)},\n",
        "$$\n",
        "то есть учит давать более высокие оценки $c_i$ элементам с большим таргетом $t_i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d4dc5b9",
      "metadata": {
        "id": "4d4dc5b9"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class CalibratedPairwiseLogistic(nn.Module):\n",
        "    def forward(self, logits, targets, lengths):\n",
        "        pairs = make_pairs(lengths)\n",
        "        targets_pairs = targets[pairs]\n",
        "        logits_pairs = logits[pairs]\n",
        "\n",
        "        w = targets_pairs[0] > targets_pairs[1]\n",
        "        ci = logits_pairs[0][w]\n",
        "        cj = logits_pairs[1][w]\n",
        "\n",
        "        if ci.numel() == 0:\n",
        "            return logits.new_tensor(0.0)\n",
        "\n",
        "        term1 = F.softplus(-ci)\n",
        "\n",
        "        log_sig_ci = -F.softplus(-ci)\n",
        "        log_sig_cj = -F.softplus(-cj)\n",
        "        term2 = torch.logaddexp(log_sig_ci, log_sig_cj)\n",
        "\n",
        "        loss = term1 + term2\n",
        "\n",
        "        #loss = F.softplus(-(ci - cj))\n",
        "\n",
        "        return torch.mean(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffc63f57",
      "metadata": {
        "id": "ffc63f57"
      },
      "source": [
        "В FinetuneModel к сырым логитам  \n",
        "$$\\ell_i = \\langle u_i, v_i\\rangle$$  \n",
        "применяется калибровка:\n",
        "\n",
        "1. Параметр «scale» (обозначим $s$) хранится в виде логарифма, то есть в модели он задан как $\\text{scale}$, а реальный множитель берётся как  \n",
        "   $$\n",
        "     \\alpha = \\exp(\\text{scale}).\n",
        "   $$\n",
        "\n",
        "2. Параметр «bias» (обозначим $b$) - это свободный смещающий коэффициент.\n",
        "\n",
        "Калиброванный логит получается по формуле  \n",
        "$$\n",
        "  \\hat\\ell_i \\;=\\; \\frac{\\ell_i}{\\alpha} \\;+\\; b\n",
        "  \\;=\\;\n",
        "  \\frac{\\langle u_i, v_i\\rangle}{\\exp(\\text{s})} \\;+\\; b.\n",
        "$$\n",
        "\n",
        "Благодаря этому механизмy модель может автоматически подстраивать и жёсткость (разброс) логитов (через $\\alpha$), и их среднее значение (через $b$), что важно для оптимальной работы попарной логистической функции потерь."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34b8ee30",
      "metadata": {
        "id": "34b8ee30"
      },
      "outputs": [],
      "source": [
        "class FinetuneModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 backbone,\n",
        "                 embedding_dim=64):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.user_context_fusion = nn.Sequential(\n",
        "            ResNet(2 * embedding_dim),\n",
        "            ResNet(2 * embedding_dim),\n",
        "            ResNet(2 * embedding_dim),\n",
        "            nn.Linear(2 * embedding_dim, embedding_dim),\n",
        "        )\n",
        "        self.candidate_projector = nn.Sequential(\n",
        "            ResNet(embedding_dim),\n",
        "            ResNet(embedding_dim),\n",
        "            ResNet(embedding_dim),\n",
        "        )\n",
        "        self._embedding_dim = embedding_dim\n",
        "        self.scale = torch.nn.Parameter(torch.zeros(1, dtype=torch.float32))\n",
        "        self.bias = torch.nn.Parameter(torch.zeros(1, dtype=torch.float32))\n",
        "        self.pairwise_loss = CalibratedPairwiseLogistic()\n",
        "\n",
        "    @property\n",
        "    def embedding_dim(self):\n",
        "        return self._embedding_dim\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        backbone_outputs = self.backbone(inputs)\n",
        "        source_embeddings = backbone_outputs['source_embeddings']\n",
        "\n",
        "        lengths = inputs['history']['lengths']\n",
        "        offsets = torch.cumsum(lengths, dim=0) - lengths\n",
        "\n",
        "        target_inds = torch.repeat_interleave(offsets, inputs['history']['targets_lengths']) + inputs['history']['targets_inds']\n",
        "        source_embeddings = source_embeddings[target_inds]\n",
        "        context_embeddings = self.backbone.context_encoder(inputs['candidates']['source_type'])\n",
        "        candidate_embeddings = self.backbone.item_encoder(inputs['candidates']['product_id'])\n",
        "\n",
        "        source_embeddings = torch.nn.functional.normalize(\n",
        "                                self.user_context_fusion(\n",
        "                                torch.cat([source_embeddings, context_embeddings], dim=-1)))\n",
        "\n",
        "        candidate_embeddings = torch.nn.functional.normalize(self.candidate_projector(candidate_embeddings))\n",
        "        source_embeddings = torch.repeat_interleave(source_embeddings, inputs['candidates']['lengths'], dim=0)\n",
        "        output_logits = torch.sum((candidate_embeddings * source_embeddings), dim=-1) / torch.exp(self.scale) + self.bias\n",
        "\n",
        "        return {\n",
        "            'logits': output_logits,\n",
        "            'loss': self.pairwise_loss(output_logits,\n",
        "                                       inputs['candidates']['action_type'],\n",
        "                                       inputs['candidates']['lengths'])\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65d6b63f",
      "metadata": {
        "id": "65d6b63f"
      },
      "outputs": [],
      "source": [
        "def test_finetune_model():\n",
        "    sample = {\n",
        "        'history': {\n",
        "            'source_type': torch.tensor([8, 8, 8, 8, 8]),\n",
        "            'action_type': torch.tensor([1, 1, 2, 2, 1]),\n",
        "            'product_id': torch.tensor([ 3551, 17044, 10396, 10396, 10396]),\n",
        "            'position': torch.tensor([0, 1, 2, 3, 4]),\n",
        "            'targets_inds': torch.tensor([1]),\n",
        "            'targets_lengths': torch.tensor([1]),\n",
        "            'lengths': torch.tensor([5])\n",
        "        },\n",
        "        'candidates': {\n",
        "            'source_type': torch.tensor([8]),\n",
        "            'action_type': torch.tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
        "            'product_id': torch.tensor([18391,  6750, 21647,  5339,  3171,  6150,  3454, 20012, 19954, 10690, 24020,  5551,  5699, 17388, 10396]),\n",
        "        'lengths': torch.tensor([15]),\n",
        "        'num_requests': torch.tensor([1])\n",
        "        }\n",
        "    }\n",
        "    backbone = ModelBackbone()\n",
        "    model_finetune = FinetuneModel(backbone)\n",
        "    output = model_finetune(sample)\n",
        "\n",
        "    assert output['logits'].shape == (15,)\n",
        "    assert output['loss'].shape == ()\n",
        "\n",
        "test_finetune_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2e01cb1",
      "metadata": {
        "id": "f2e01cb1"
      },
      "source": [
        "## 10. Обучаем finetune модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "615e6b46",
      "metadata": {
        "id": "615e6b46"
      },
      "outputs": [],
      "source": [
        "def train_finetune_model(model, train_loader, valid_loader, optimizer, scheduler, num_epochs, device):\n",
        "    prev_valid_ndcg = None\n",
        "    global_cnt = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for batch in tqdm(train_loader):\n",
        "            batch = move_to_device(batch, device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = model(batch)['loss']\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "        scheduler.step()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {mean(train_losses):.6f}\")\n",
        "\n",
        "        model.eval()\n",
        "        valid_losses = []\n",
        "        valid_logits = []\n",
        "        valid_targets = []\n",
        "        with torch.inference_mode():\n",
        "            for batch in tqdm(valid_loader):\n",
        "                batch = move_to_device(batch, device)\n",
        "                output = model(batch)\n",
        "                loss = output['loss']\n",
        "                valid_losses.append(loss.item())\n",
        "                logits = output['logits']\n",
        "                targets = batch['candidates']['action_type']\n",
        "                lengths = batch['candidates']['lengths']\n",
        "                i = 0\n",
        "                for length in lengths:\n",
        "                    if length > 1:\n",
        "                        valid_logits.append(logits[i:i + length].cpu().numpy())\n",
        "                        valid_targets.append(targets[i:i + length].cpu().numpy())\n",
        "                    i += length\n",
        "\n",
        "        avg_valid_ndcg = 0\n",
        "        for logits, targets in zip(valid_logits, valid_targets):\n",
        "            avg_valid_ndcg += ndcg_score(targets[None,], logits[None,], k=10, ignore_ties=True)\n",
        "        avg_valid_ndcg /= len(valid_logits)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Valid Loss: {mean(valid_losses):.6f}\")\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Valid NDCG@10: {avg_valid_ndcg}\")\n",
        "\n",
        "        if prev_valid_ndcg is None or prev_valid_ndcg < avg_valid_ndcg:\n",
        "            global_cnt = 0\n",
        "            prev_valid_ndcg = avg_valid_ndcg\n",
        "            with torch.no_grad():\n",
        "                torch.save(model, './finetune.pt')\n",
        "        else:\n",
        "            global_cnt += 1\n",
        "            if global_cnt == 10:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38e451a2",
      "metadata": {
        "id": "38e451a2"
      },
      "outputs": [],
      "source": [
        "lr = 0.001\n",
        "batch_size = 4\n",
        "warmup_epochs = 6\n",
        "start_factor = 0.1\n",
        "num_epochs = 100\n",
        "\n",
        "embedding_dim = 64\n",
        "num_heads = 2\n",
        "max_seq_len = 512\n",
        "dropout_rate = 0.1\n",
        "num_transformer_layers = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48982863",
      "metadata": {
        "id": "48982863"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0ca8060",
      "metadata": {
        "id": "e0ca8060"
      },
      "outputs": [],
      "source": [
        "backbone = ModelBackbone(embedding_dim=embedding_dim,\n",
        "                        num_heads=num_heads,\n",
        "                        max_seq_len=max_seq_len,\n",
        "                        dropout_rate=dropout_rate,\n",
        "                        num_transformer_layers=num_transformer_layers).to(device)\n",
        "\n",
        "model_finetune = FinetuneModel(backbone=backbone,\n",
        "                             embedding_dim=embedding_dim).to(device).to(device)\n",
        "optimizer = optim.AdamW(model_finetune.parameters(), lr=lr, weight_decay=0.01)\n",
        "scheduler = optim.lr_scheduler.LinearLR(\n",
        "    optimizer,\n",
        "    start_factor=start_factor,\n",
        "    total_iters=warmup_epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "269f6509",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "269f6509",
        "outputId": "b254d6ca-607c-423b-cf04-8f6a10bd91b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 59.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.694108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Valid Loss: 0.693395\n",
            "Epoch 1/100, Valid NDCG@10: 0.2546861600107447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 68.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100, Train Loss: 0.692111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100, Valid Loss: 0.693322\n",
            "Epoch 2/100, Valid NDCG@10: 0.2570427670724341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 65.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100, Train Loss: 0.688936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100, Valid Loss: 0.693499\n",
            "Epoch 3/100, Valid NDCG@10: 0.25713644874995256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100, Train Loss: 0.684719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100, Valid Loss: 0.693407\n",
            "Epoch 4/100, Valid NDCG@10: 0.25852186271025285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 68.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100, Train Loss: 0.676360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100, Valid Loss: 0.694664\n",
            "Epoch 5/100, Valid NDCG@10: 0.2616602168785861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100, Train Loss: 0.662996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100, Valid Loss: 0.695768\n",
            "Epoch 6/100, Valid NDCG@10: 0.2617906381541963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 66.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100, Train Loss: 0.642791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100, Valid Loss: 0.696026\n",
            "Epoch 7/100, Valid NDCG@10: 0.26647013288501536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100, Train Loss: 0.615586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100, Valid Loss: 0.698590\n",
            "Epoch 8/100, Valid NDCG@10: 0.2713029140449932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 65.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100, Train Loss: 0.579750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 38.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100, Valid Loss: 0.706566\n",
            "Epoch 9/100, Valid NDCG@10: 0.268429292915627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 68.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100, Train Loss: 0.539574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 38.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100, Valid Loss: 0.713357\n",
            "Epoch 10/100, Valid NDCG@10: 0.276569378244181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100, Train Loss: 0.493453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100, Valid Loss: 0.733010\n",
            "Epoch 11/100, Valid NDCG@10: 0.2744708748629545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 66.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100, Train Loss: 0.447443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 38.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100, Valid Loss: 0.753066\n",
            "Epoch 12/100, Valid NDCG@10: 0.27648931996836923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100, Train Loss: 0.403074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100, Valid Loss: 0.783191\n",
            "Epoch 13/100, Valid NDCG@10: 0.27505940661850004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100, Train Loss: 0.361130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100, Valid Loss: 0.814066\n",
            "Epoch 14/100, Valid NDCG@10: 0.27694571772869075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 68.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100, Train Loss: 0.326589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 32.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100, Valid Loss: 0.851196\n",
            "Epoch 15/100, Valid NDCG@10: 0.2765302838861126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 68.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100, Train Loss: 0.292372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100, Valid Loss: 0.891140\n",
            "Epoch 16/100, Valid NDCG@10: 0.2770081934039496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 66.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100, Train Loss: 0.263815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100, Valid Loss: 0.937039\n",
            "Epoch 17/100, Valid NDCG@10: 0.27672211828648846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 65.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100, Train Loss: 0.239859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 38.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100, Valid Loss: 0.976588\n",
            "Epoch 18/100, Valid NDCG@10: 0.2759399086248945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/100, Train Loss: 0.217944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 38.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/100, Valid Loss: 1.020218\n",
            "Epoch 19/100, Valid NDCG@10: 0.27831837665400627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 66.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/100, Train Loss: 0.198325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/100, Valid Loss: 1.065931\n",
            "Epoch 20/100, Valid NDCG@10: 0.2771304165878406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 69.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/100, Train Loss: 0.183882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 38.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/100, Valid Loss: 1.105885\n",
            "Epoch 21/100, Valid NDCG@10: 0.2777282734656781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 68.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/100, Train Loss: 0.168667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/100, Valid Loss: 1.146273\n",
            "Epoch 22/100, Valid NDCG@10: 0.27696814350310045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 68.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/100, Train Loss: 0.157582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 38.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/100, Valid Loss: 1.180521\n",
            "Epoch 23/100, Valid NDCG@10: 0.27702362427335275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 68.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/100, Train Loss: 0.145772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/100, Valid Loss: 1.217736\n",
            "Epoch 24/100, Valid NDCG@10: 0.277624958646671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/100, Train Loss: 0.136098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/100, Valid Loss: 1.262788\n",
            "Epoch 25/100, Valid NDCG@10: 0.2796917890621476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/100, Train Loss: 0.128355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/100, Valid Loss: 1.290439\n",
            "Epoch 26/100, Valid NDCG@10: 0.2801923030409099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 65.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/100, Train Loss: 0.118789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 38.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/100, Valid Loss: 1.322291\n",
            "Epoch 27/100, Valid NDCG@10: 0.281143559199074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 68.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/100, Train Loss: 0.114026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/100, Valid Loss: 1.359577\n",
            "Epoch 28/100, Valid NDCG@10: 0.27960267665894967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/100, Train Loss: 0.108824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 32.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/100, Valid Loss: 1.394563\n",
            "Epoch 29/100, Valid NDCG@10: 0.28024568728227045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/100, Train Loss: 0.101837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 38.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/100, Valid Loss: 1.412267\n",
            "Epoch 30/100, Valid NDCG@10: 0.2787179308377197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/100, Train Loss: 0.095012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/100, Valid Loss: 1.435590\n",
            "Epoch 31/100, Valid NDCG@10: 0.2831360686156984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 68.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/100, Train Loss: 0.091292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/100, Valid Loss: 1.465331\n",
            "Epoch 32/100, Valid NDCG@10: 0.2798770118282553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 68.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/100, Train Loss: 0.086790\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/100, Valid Loss: 1.475408\n",
            "Epoch 33/100, Valid NDCG@10: 0.2817704699945992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 68.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/100, Train Loss: 0.081552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 38.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/100, Valid Loss: 1.506873\n",
            "Epoch 34/100, Valid NDCG@10: 0.28079910201217434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 68.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/100, Train Loss: 0.079628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/100, Valid Loss: 1.530875\n",
            "Epoch 35/100, Valid NDCG@10: 0.2819948348623824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 65.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/100, Train Loss: 0.075387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/100, Valid Loss: 1.534424\n",
            "Epoch 36/100, Valid NDCG@10: 0.2851178730571711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/100, Train Loss: 0.072174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/100, Valid Loss: 1.576321\n",
            "Epoch 37/100, Valid NDCG@10: 0.2834175607235177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 66.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/100, Train Loss: 0.069576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/100, Valid Loss: 1.586873\n",
            "Epoch 38/100, Valid NDCG@10: 0.28079375741346896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 68.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/100, Train Loss: 0.066024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/100, Valid Loss: 1.608439\n",
            "Epoch 39/100, Valid NDCG@10: 0.2813023848924957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 66.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/100, Train Loss: 0.063204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/100, Valid Loss: 1.632899\n",
            "Epoch 40/100, Valid NDCG@10: 0.28328605454411476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 68.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/100, Train Loss: 0.060043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/100, Valid Loss: 1.642190\n",
            "Epoch 41/100, Valid NDCG@10: 0.2836053274862247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 68.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/100, Train Loss: 0.056804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 38.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/100, Valid Loss: 1.669807\n",
            "Epoch 42/100, Valid NDCG@10: 0.28271136990400586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/100, Train Loss: 0.056408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 32.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/100, Valid Loss: 1.700580\n",
            "Epoch 43/100, Valid NDCG@10: 0.2823041095260559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/100, Train Loss: 0.053669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/100, Valid Loss: 1.715311\n",
            "Epoch 44/100, Valid NDCG@10: 0.2797988911833067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 66.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/100, Train Loss: 0.050857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/100, Valid Loss: 1.738091\n",
            "Epoch 45/100, Valid NDCG@10: 0.2788766263616418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/100, Train Loss: 0.048640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/100, Valid Loss: 1.753403\n",
            "Epoch 46/100, Valid NDCG@10: 0.2813816070701094\n"
          ]
        }
      ],
      "source": [
        "train_finetune_model(model_finetune, train_loader, valid_loader, optimizer, scheduler, num_epochs, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0229cdc8",
      "metadata": {
        "id": "0229cdc8"
      },
      "source": [
        "Пробуем инициализироваться предобученной моделью, только backbone:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a77b8b1",
      "metadata": {
        "id": "3a77b8b1"
      },
      "outputs": [],
      "source": [
        "model_pretrain = torch.load('./pretrain.pt', weights_only=False)\n",
        "model_finetune = FinetuneModel(model_pretrain.backbone, embedding_dim).to(device)\n",
        "optimizer = optim.AdamW(model_finetune.parameters(), lr=lr, weight_decay=0.01)\n",
        "scheduler = optim.lr_scheduler.LinearLR(\n",
        "    optimizer,\n",
        "    start_factor=start_factor,\n",
        "    total_iters=warmup_epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c79b3af9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c79b3af9",
        "outputId": "aeea2adf-84f8-4ab7-ea79-69b8e4f886d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 65.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.691471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Valid Loss: 0.690597\n",
            "Epoch 1/100, Valid NDCG@10: 0.2715581525414558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 66.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100, Train Loss: 0.686504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 38.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100, Valid Loss: 0.688594\n",
            "Epoch 2/100, Valid NDCG@10: 0.2853143746444059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 66.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100, Train Loss: 0.682444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100, Valid Loss: 0.687203\n",
            "Epoch 3/100, Valid NDCG@10: 0.29198543922684267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 66.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100, Train Loss: 0.675731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100, Valid Loss: 0.685760\n",
            "Epoch 4/100, Valid NDCG@10: 0.2946631830565442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100, Train Loss: 0.663543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100, Valid Loss: 0.682968\n",
            "Epoch 5/100, Valid NDCG@10: 0.2980635697373388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100, Train Loss: 0.646557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100, Valid Loss: 0.681894\n",
            "Epoch 6/100, Valid NDCG@10: 0.3027882754625199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100, Train Loss: 0.622757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100, Valid Loss: 0.681384\n",
            "Epoch 7/100, Valid NDCG@10: 0.2993919543717133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100, Train Loss: 0.592705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 38.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100, Valid Loss: 0.680859\n",
            "Epoch 8/100, Valid NDCG@10: 0.30762836971960333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 65.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100, Train Loss: 0.557855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 38.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100, Valid Loss: 0.688612\n",
            "Epoch 9/100, Valid NDCG@10: 0.30201546988229494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 64.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100, Train Loss: 0.518535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100, Valid Loss: 0.698482\n",
            "Epoch 10/100, Valid NDCG@10: 0.2975723309701491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100, Train Loss: 0.479594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 32.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100, Valid Loss: 0.710182\n",
            "Epoch 11/100, Valid NDCG@10: 0.3031960916175086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100, Train Loss: 0.441269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 36.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100, Valid Loss: 0.733430\n",
            "Epoch 12/100, Valid NDCG@10: 0.30074281482762627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 69.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100, Train Loss: 0.402006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100, Valid Loss: 0.758549\n",
            "Epoch 13/100, Valid NDCG@10: 0.30010136594632275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 66.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100, Train Loss: 0.368085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100, Valid Loss: 0.788902\n",
            "Epoch 14/100, Valid NDCG@10: 0.29746587528192325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100, Train Loss: 0.333925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 38.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100, Valid Loss: 0.823396\n",
            "Epoch 15/100, Valid NDCG@10: 0.297276212739027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 66.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100, Train Loss: 0.306275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100, Valid Loss: 0.849111\n",
            "Epoch 16/100, Valid NDCG@10: 0.29814354090040174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100, Train Loss: 0.280859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100, Valid Loss: 0.894657\n",
            "Epoch 17/100, Valid NDCG@10: 0.301113224742707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:01<00:00, 67.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100, Train Loss: 0.258980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44/44 [00:01<00:00, 37.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100, Valid Loss: 0.932736\n",
            "Epoch 18/100, Valid NDCG@10: 0.2962556867154115\n"
          ]
        }
      ],
      "source": [
        "train_finetune_model(model_finetune, train_loader, valid_loader, optimizer, scheduler, num_epochs, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60d7521a",
      "metadata": {
        "id": "60d7521a"
      },
      "source": [
        "Попробуем еще дополнительно иницилизировать user_context_fusion и candidate_projector:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28d2cf28",
      "metadata": {
        "id": "28d2cf28"
      },
      "outputs": [],
      "source": [
        "model_pretrain = torch.load('./pretrain.pt', weights_only=False)\n",
        "model_finetune = FinetuneModel(model_pretrain.backbone, embedding_dim).to(device)\n",
        "model_finetune.user_context_fusion = model_pretrain.user_context_fusion\n",
        "model_finetune.candidate_projector = model_pretrain.candidate_projector\n",
        "\n",
        "optimizer = optim.AdamW(model_finetune.parameters(), lr=lr, weight_decay=0.01)\n",
        "scheduler = optim.lr_scheduler.LinearLR(\n",
        "    optimizer,\n",
        "    start_factor=start_factor,\n",
        "    total_iters=warmup_epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c9ae51d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0c9ae51d",
        "outputId": "8b0e6658-0f5c-408a-e7cc-c26275e70095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:03<00:00, 59.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.679132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 59.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Valid Loss: 0.682856\n",
            "Epoch 1/100, Valid NDCG@10: 0.30732482430123936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:03<00:00, 59.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100, Train Loss: 0.669400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 60.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100, Valid Loss: 0.679741\n",
            "Epoch 2/100, Valid NDCG@10: 0.30849815430583744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:03<00:00, 60.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100, Train Loss: 0.659003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 61.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100, Valid Loss: 0.678653\n",
            "Epoch 3/100, Valid NDCG@10: 0.30931249287854684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:03<00:00, 61.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100, Train Loss: 0.644037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 62.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100, Valid Loss: 0.678889\n",
            "Epoch 4/100, Valid NDCG@10: 0.31046323207923315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:03<00:00, 61.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100, Train Loss: 0.625504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 60.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100, Valid Loss: 0.683930\n",
            "Epoch 5/100, Valid NDCG@10: 0.31124725443319823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:03<00:00, 60.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100, Train Loss: 0.601912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 60.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100, Valid Loss: 0.694785\n",
            "Epoch 6/100, Valid NDCG@10: 0.3086770732148856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:03<00:00, 61.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100, Train Loss: 0.580264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 61.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100, Valid Loss: 0.705092\n",
            "Epoch 7/100, Valid NDCG@10: 0.3069931432152151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:03<00:00, 59.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100, Train Loss: 0.557128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 60.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100, Valid Loss: 0.730049\n",
            "Epoch 8/100, Valid NDCG@10: 0.30959840999771815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:03<00:00, 61.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100, Train Loss: 0.532404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 60.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100, Valid Loss: 0.759088\n",
            "Epoch 9/100, Valid NDCG@10: 0.3093430684839358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:03<00:00, 61.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100, Train Loss: 0.505664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 61.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100, Valid Loss: 0.792779\n",
            "Epoch 10/100, Valid NDCG@10: 0.3062908252445432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:03<00:00, 59.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100, Train Loss: 0.476132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 59.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100, Valid Loss: 0.806685\n",
            "Epoch 11/100, Valid NDCG@10: 0.3092291720752101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:03<00:00, 61.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100, Train Loss: 0.440795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 61.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100, Valid Loss: 0.855497\n",
            "Epoch 12/100, Valid NDCG@10: 0.30261216950507674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:03<00:00, 59.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100, Train Loss: 0.405742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 60.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100, Valid Loss: 0.878947\n",
            "Epoch 13/100, Valid NDCG@10: 0.3052794059313995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:03<00:00, 60.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100, Train Loss: 0.372725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 60.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100, Valid Loss: 0.933721\n",
            "Epoch 14/100, Valid NDCG@10: 0.30291167875036706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 208/208 [00:03<00:00, 59.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100, Train Loss: 0.343517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 50.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100, Valid Loss: 0.995619\n",
            "Epoch 15/100, Valid NDCG@10: 0.30357367883426856\n"
          ]
        }
      ],
      "source": [
        "train_finetune_model(model_finetune, train_loader, valid_loader, optimizer, scheduler, num_epochs, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f58b7aa5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "f58b7aa5",
        "outputId": "70518227-048e-47d2-c105-24bb356f5527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 59.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.31124725443319823\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3853438077.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mavg_valid_ndcg\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.324\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtest_finetune_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3853438077.py\u001b[0m in \u001b[0;36mtest_finetune_model\u001b[0;34m(valid_loader)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mavg_valid_ndcg\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_valid_ndcg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mavg_valid_ndcg\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.324\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mtest_finetune_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def best_metrics_finetune_model(valid_loader):\n",
        "    valid_logits = []\n",
        "    valid_targets = []\n",
        "    with torch.inference_mode():\n",
        "        test_model = torch.load('./finetune.pt', weights_only=False)\n",
        "        for batch in tqdm(valid_loader):\n",
        "            batch = move_to_device(batch, device)\n",
        "            output = test_model(batch)\n",
        "            logits = output['logits']\n",
        "            targets = batch['candidates']['action_type']\n",
        "            lengths = batch['candidates']['lengths']\n",
        "            i = 0\n",
        "            for length in lengths:\n",
        "                if length > 1:\n",
        "                    valid_logits.append(logits[i:i + length].cpu().numpy())\n",
        "                    valid_targets.append(targets[i:i + length].cpu().numpy())\n",
        "                i += length\n",
        "\n",
        "    avg_valid_ndcg = 0\n",
        "    for logits, targets in zip(valid_logits, valid_targets):\n",
        "        avg_valid_ndcg += ndcg_score(targets[None,], logits[None,], k=10, ignore_ties=True)\n",
        "    avg_valid_ndcg /= len(valid_logits)\n",
        "    print(avg_valid_ndcg)\n",
        "\n",
        "test_finetune_model(valid_loader)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}